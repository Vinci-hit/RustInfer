# RustInfer å¼€å‘è€…æŒ‡å—

æœ¬æ–‡æ¡£ä¸º RustInfer é¡¹ç›®çš„å¼€å‘è€…å’Œæ½œåœ¨è´¡çŒ®è€…æä¾›æ·±åº¦æŠ€æœ¯æŒ‡å—ï¼Œè¯¦ç»†è§£é‡Šæ ¸å¿ƒè®¾è®¡ç†å¿µã€æ¶æ„å†³ç­–å’Œæœ€ä½³å®è·µã€‚

---

## ç›®å½•

1. [è®¾è®¡å“²å­¦](#è®¾è®¡å“²å­¦)
2. [æ ¸å¿ƒæ¶æ„](#æ ¸å¿ƒæ¶æ„)
3. [Tensor ç³»ç»Ÿï¼šç±»å‹å®‰å…¨çš„å¼ é‡æŠ½è±¡](#tensor-ç³»ç»Ÿç±»å‹å®‰å…¨çš„å¼ é‡æŠ½è±¡)
4. [è‡ªåŠ¨èµ„æºç®¡ç†ï¼šRAII ä¸ Drop Trait](#è‡ªåŠ¨èµ„æºç®¡ç†raii-ä¸-drop-trait)
5. [Op Traitï¼šç»Ÿä¸€çš„ç®—å­æ¥å£](#op-traitç»Ÿä¸€çš„ç®—å­æ¥å£)
6. [Model Traitï¼šæ¨¡å‹æŠ½è±¡å±‚](#model-traitæ¨¡å‹æŠ½è±¡å±‚)
7. [è®¾å¤‡æŠ½è±¡ï¼šCPU ä¸ CUDA ç»Ÿä¸€æ¥å£](#è®¾å¤‡æŠ½è±¡cpu-ä¸-cuda-ç»Ÿä¸€æ¥å£)
8. [å†…å­˜æ± åŒ–ï¼šCachingCudaAllocator](#å†…å­˜æ± åŒ–cachingcudaallocator)
9. [KV Cache ç®¡ç†ï¼šé›¶æ‹·è´è§†å›¾](#kv-cache-ç®¡ç†é›¶æ‹·è´è§†å›¾)
10. [Workspace æ¨¡å¼ï¼šé¢„åˆ†é…å†…å­˜](#workspace-æ¨¡å¼é¢„åˆ†é…å†…å­˜)
11. [é›¶æ‹·è´æƒé‡åŠ è½½](#é›¶æ‹·è´æƒé‡åŠ è½½)
12. [æ€§èƒ½ä¼˜åŒ–ç­–ç•¥](#æ€§èƒ½ä¼˜åŒ–ç­–ç•¥)
13. [å¼€å‘æŒ‡å—](#å¼€å‘æŒ‡å—)
14. [è°ƒè¯•ä¸åˆ†æ](#è°ƒè¯•ä¸åˆ†æ)

---

## è®¾è®¡å“²å­¦

RustInfer çš„æ¶æ„å›´ç»•ä¸‰ä¸ªæ ¸å¿ƒåŸåˆ™æ„å»ºï¼š

### 1. **RAII (Resource Acquisition Is Initialization)**
> "èµ„æºçš„è·å–å³åˆå§‹åŒ–ï¼Œèµ„æºçš„é‡Šæ”¾å³ææ„"

Rust çš„æ‰€æœ‰æƒç³»ç»Ÿå’Œ `Drop` trait ä½¿å¾—èµ„æºç®¡ç†å˜å¾—è‡ªåŠ¨ä¸”ç¡®å®šæ€§ã€‚åœ¨ RustInfer ä¸­ï¼š
- CUDA å†…å­˜åœ¨ `Buffer` é”€æ¯æ—¶è‡ªåŠ¨é‡Šæ”¾
- CUDA æµã€å¥æŸ„åœ¨ `CudaConfig` é”€æ¯æ—¶è‡ªåŠ¨æ¸…ç†
- ä¸å¯èƒ½å‡ºç°å†…å­˜æ³„æ¼ã€åŒé‡é‡Šæ”¾æˆ–æ‚¬å‚æŒ‡é’ˆ

**ä¸ºä»€ä¹ˆé‡è¦**ï¼šä¼ ç»Ÿ C++ æ¨ç†å¼•æ“ä¸­ï¼Œå¿˜è®°è°ƒç”¨ `cudaFree` æ˜¯å¸¸è§ bugã€‚RAII ä»æ ¹æœ¬ä¸Šæ¶ˆé™¤äº†è¿™ç±»é—®é¢˜ã€‚

### 2. **é›¶æˆæœ¬æŠ½è±¡ (Zero-Cost Abstractions)**
> "ä½ ä¸éœ€è¦ä¸ºä½ ä¸ä½¿ç”¨çš„åŠŸèƒ½ä»˜å‡ºä»£ä»·"

RustInfer ä½¿ç”¨ Rust çš„æ³›å‹ã€trait å’Œæšä¸¾æ¥å®ç°æŠ½è±¡ï¼Œä½†è¿™äº›æŠ½è±¡åœ¨ç¼–è¯‘åç­‰ä»·äºæ‰‹å†™çš„ C ä»£ç ï¼š
- `Tensor` æšä¸¾ä¼šè¢«ç¼–è¯‘å™¨å•æ€åŒ–ï¼ˆmonomorphizationï¼‰
- Trait å¯¹è±¡çš„åŠ¨æ€åˆ†å‘ä»…åœ¨å¿…è¦æ—¶ä½¿ç”¨ï¼ˆå¦‚ `Tokenizer`ï¼‰
- å†…è”å’Œå¸¸é‡ä¼ æ’­æ¶ˆé™¤äº†æŠ½è±¡å¼€é”€

**ä¸ºä»€ä¹ˆé‡è¦**ï¼šå¯ä»¥ç¼–å†™é«˜å±‚æ¬¡ã€æ˜“ç»´æŠ¤çš„ä»£ç ï¼ŒåŒæ—¶ä¿æŒ C++ çº§åˆ«çš„æ€§èƒ½ã€‚

### 3. **ç±»å‹é©±åŠ¨çš„æ­£ç¡®æ€§ (Type-Driven Correctness)**
> "è®©éæ³•çŠ¶æ€ä¸å¯è¡¨ç¤º"

Rust çš„ç±»å‹ç³»ç»Ÿåœ¨ç¼–è¯‘æ—¶é˜²æ­¢é”™è¯¯ï¼š
- ä¸èƒ½åœ¨ CPU tensor ä¸Šè°ƒç”¨ CUDA æ“ä½œ
- ä¸èƒ½æ··åˆä¸åŒæ•°æ®ç±»å‹çš„ tensor
- ä¸èƒ½åœ¨ä¸åŒè®¾å¤‡é—´é”™è¯¯ä¼ é€’æ•°æ®

**ä¸ºä»€ä¹ˆé‡è¦**ï¼šåœ¨æ¨ç†å¼•æ“ä¸­ï¼Œè®¾å¤‡/æ•°æ®ç±»å‹é”™è¯¯å¯èƒ½å¯¼è‡´éš¾ä»¥è°ƒè¯•çš„ CUDA é”™è¯¯æˆ–é™é»˜é”™è¯¯ã€‚ç±»å‹ç³»ç»Ÿå°†è¿è¡Œæ—¶é”™è¯¯å˜ä¸ºç¼–è¯‘æ—¶é”™è¯¯ã€‚

---

## æ ¸å¿ƒæ¶æ„

RustInfer é‡‡ç”¨æ¨¡å—åŒ–ã€åˆ†å±‚è®¾è®¡ï¼š

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          Model Trait (llama3.rs)            â”‚  â† ç”¨æˆ·æ¥å£
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     Op Trait (rmsnorm, matmul, flash_attn)  â”‚  â† ç®—å­å±‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Tensor (TypedTensor<T> + enum Tensor)      â”‚  â† æ•°æ®æŠ½è±¡
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Buffer (Arc<BufferInner> + è§†å›¾)           â”‚  â† å†…å­˜ç®¡ç†
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  DeviceAllocator (CPU/CachingCudaAllocator) â”‚  â† åˆ†é…å™¨
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  CUDA FFI / CPU kernels                     â”‚  â† ç¡¬ä»¶å±‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**åˆ†å±‚ä¼˜åŠ¿**ï¼š
- **éš”ç¦»å˜åŒ–**ï¼šæ›´æ¢æ¨¡å‹ä¸å½±å“ç®—å­ï¼Œæ›´æ¢ç®—å­ä¸å½±å“ tensor ç³»ç»Ÿ
- **å¯æµ‹è¯•**ï¼šæ¯ä¸€å±‚å¯ä»¥ç‹¬ç«‹æµ‹è¯•
- **å¯æ‰©å±•**ï¼šæ·»åŠ æ–°è®¾å¤‡ï¼ˆå¦‚ Metalã€Vulkanï¼‰åªéœ€å®ç° `DeviceAllocator`

---

## Tensor ç³»ç»Ÿï¼šç±»å‹å®‰å…¨çš„å¼ é‡æŠ½è±¡

### è®¾è®¡æ¦‚è§ˆ

Tensor ç³»ç»Ÿé‡‡ç”¨**ä¸‰å±‚è®¾è®¡**ï¼Œå¹³è¡¡äº†ç±»å‹å®‰å…¨å’Œè¿è¡Œæ—¶çµæ´»æ€§ï¼š

```rust
// ç¬¬ä¸€å±‚ï¼šTrait å®šä¹‰åˆæ³•ç±»å‹
pub trait Dtype: Send + Sync + Copy + 'static {
    const DTYPE: DataType;
}

// ç¬¬äºŒå±‚ï¼šæ³›å‹ Tensorï¼Œç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥
pub struct TypedTensor<T: Dtype> {
    dims: Arc<[usize]>,           // å½¢çŠ¶ï¼ˆArc ä½¿å…‹éš†å»‰ä»·ï¼‰
    num_elements: usize,          // ç¼“å­˜çš„å…ƒç´ æ€»æ•°
    buffer: Buffer,               // åº•å±‚å­˜å‚¨
    _phantom: PhantomData<T>,     // é›¶å¼€é”€çš„ç±»å‹æ ‡è®°
}

// ç¬¬ä¸‰å±‚ï¼šæšä¸¾åŒ…è£…ï¼Œè¿è¡Œæ—¶å¤šæ€
pub enum Tensor {
    F32(TypedTensor<f32>),
    I32(TypedTensor<i32>),
    BF16(TypedTensor<bf16>),
    // ...
}
```

### ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ

#### é—®é¢˜1ï¼šå¦‚ä½•åŒæ—¶æ”¯æŒç¼–è¯‘æ—¶ç±»å‹æ£€æŸ¥å’Œè¿è¡Œæ—¶ç±»å‹çµæ´»æ€§ï¼Ÿ

**çŸ›ç›¾**ï¼š
- æ¨¡å‹æƒé‡çš„ç±»å‹åœ¨ç¼–è¯‘æ—¶æœªçŸ¥ï¼ˆBF16ã€FP32ã€INT8 å–å†³äºé…ç½®ï¼‰
- ä½†æˆ‘ä»¬å¸Œæœ›åœ¨ç¼–è¯‘æ—¶ä¿è¯ç±»å‹å®‰å…¨ï¼ˆå¦‚ä¸èƒ½å°† FP32 tensor ä¼ ç»™æœŸæœ› BF16 çš„ç®—å­ï¼‰

**è§£å†³æ–¹æ¡ˆ**ï¼š
```rust
// åœ¨ç®—å­å†…éƒ¨ï¼Œä½¿ç”¨ TypedTensor<T> æä¾›ç¼–è¯‘æ—¶ä¿è¯
impl RMSNorm {
    fn forward(&self, input: &Tensor) -> Result<Tensor> {
        // è¿è¡Œæ—¶æ£€æŸ¥ç±»å‹
        match input {
            Tensor::BF16(typed_input) => {
                // æ­¤å¤„ typed_input çš„ç±»å‹æ˜¯ TypedTensor<bf16>
                // ç¼–è¯‘å™¨ç¡®ä¿æˆ‘ä»¬åªèƒ½è°ƒç”¨ bf16 ç›¸å…³çš„æ“ä½œ
                self.forward_bf16(typed_input)
            }
            _ => Err(Error::UnsupportedDtype)
        }
    }
}
```

#### é—®é¢˜2ï¼šä¸ºä»€ä¹ˆä½¿ç”¨ `PhantomData<T>`ï¼Ÿ

`PhantomData` æ˜¯é›¶å¼€é”€çš„ç±»å‹æ ‡è®°ï¼Œå‘Šè¯‰ç¼–è¯‘å™¨ `TypedTensor` "æ‹¥æœ‰" ç±»å‹ `T` çš„æ•°æ®ã€‚è¿™ç¡®ä¿ï¼š

1. **å‹å˜ï¼ˆVarianceï¼‰æ­£ç¡®æ€§**ï¼š`TypedTensor<&'a T>` çš„ç”Ÿå‘½å‘¨æœŸè§„åˆ™æ­£ç¡®
2. **Drop æ£€æŸ¥**ï¼šå¦‚æœ `T: Drop`ï¼Œç¼–è¯‘å™¨ä¼šå¼ºåˆ¶ `TypedTensor` ä¹Ÿå®ç° Drop
3. **Send/Sync ä¼ æ’­**ï¼š`T: Send` æ¨å¯¼ `TypedTensor<T>: Send`

```rust
// å¦‚æœæ²¡æœ‰ PhantomDataï¼Œè¿™æ®µä»£ç ä¼šç¼–è¯‘é€šè¿‡ä½†å¯¼è‡´ UBï¼š
let tensor: TypedTensor<f32> = ...; // å®é™…å­˜å‚¨ f32
let tensor_i32: TypedTensor<i32> = unsafe { std::mem::transmute(tensor) }; // é”™è¯¯çš„ç±»å‹è§£é‡Šï¼

// æœ‰äº† PhantomDataï¼Œtransmute ä¼šå› ä¸ºç±»å‹ä¸åŒ¹é…è€Œç¼–è¯‘å¤±è´¥
```

#### é—®é¢˜3ï¼šä¸ºä»€ä¹ˆ `dims` ç”¨ `Arc<[usize]>` è€Œä¸æ˜¯ `Vec<usize>`ï¼Ÿ

**åŸå› **ï¼š
1. **å»‰ä»·å…‹éš†**ï¼š`Tensor::clone()` åªéœ€å…‹éš† `Arc`ï¼ˆåŸå­å¼•ç”¨è®¡æ•°åŠ 1ï¼‰ï¼Œè€Œä¸æ˜¯æ‹·è´æ•´ä¸ª shape
2. **å…±äº«å½¢çŠ¶**ï¼šå¤šä¸ª tensor view å¯ä»¥å…±äº«åŒä¸€ä¸ª shapeï¼ˆå¯¹äº reshapeã€transpose ç­‰æ“ä½œå¾ˆæœ‰ç”¨ï¼‰
3. **ä¸å¯å˜æ€§**ï¼š`Arc<[usize]>` æ˜¯ä¸å¯å˜çš„ï¼Œé˜²æ­¢æ„å¤–ä¿®æ”¹ shape

**æ€§èƒ½æ•°æ®**ï¼š
```
Vec<usize> clone:  ~50nsï¼ˆæ‹·è´ 4 ä¸ª usizeï¼‰
Arc<[usize]> clone: ~3nsï¼ˆåŸå­åŠ æ³• + æŒ‡é’ˆæ‹·è´ï¼‰
```

### TypedTensor çš„æ ¸å¿ƒæ–¹æ³•

#### å®‰å…¨çš„ CPU æ•°æ®è®¿é—®

```rust
impl<T: Dtype> TypedTensor<T> {
    pub fn as_slice(&self) -> Result<&[T]> {
        // 1. è¿è¡Œæ—¶æ£€æŸ¥ï¼šå¿…é¡»åœ¨ CPU ä¸Š
        if self.buffer.device() != DeviceType::Cpu {
            return Err(Error::DeviceMismatch { ... });
        }

        // 2. å®‰å…¨åœ°ä»è£¸æŒ‡é’ˆé‡å»ºåˆ‡ç‰‡
        unsafe {
            let ptr = self.buffer.as_ptr() as *const T;
            Ok(std::slice::from_raw_parts(ptr, self.num_elements))
        }
    }
}
```

**ä¸ºä»€ä¹ˆå®‰å…¨**ï¼š
- `Buffer` ä¿è¯æŒ‡é’ˆåœ¨ buffer ç”Ÿå‘½å‘¨æœŸå†…æœ‰æ•ˆ
- è®¾å¤‡æ£€æŸ¥é˜²æ­¢è®¿é—® GPU å†…å­˜
- åˆ‡ç‰‡é•¿åº¦ç”± `num_elements` ä¿è¯æ­£ç¡®

#### é›¶æ‹·è´åˆ‡ç‰‡

```rust
pub fn slice(&self, offsets: &[usize], lengths: &[usize]) -> Result<Self> {
    // è®¡ç®—æ–°è§†å›¾çš„å­—èŠ‚åç§»
    let byte_offset = calculate_offset(offsets, &self.dims) * std::mem::size_of::<T>();

    // åˆ›å»ºæ–° buffer è§†å›¾ï¼ˆå…±äº«åº•å±‚å†…å­˜ï¼‰
    let sliced_buffer = self.buffer.slice(byte_offset, new_size_bytes)?;

    Ok(Self {
        dims: Arc::from(lengths),
        num_elements: lengths.iter().product(),
        buffer: sliced_buffer,  // Arc cloneï¼Œä¸æ‹·è´æ•°æ®
        _phantom: PhantomData,
    })
}
```

**åº”ç”¨åœºæ™¯**ï¼š
- KV cache åˆ‡ç‰‡ï¼ˆå–å‰ N ä¸ª token çš„ K/Vï¼‰
- Batch å¤„ç†ï¼ˆå–ç¬¬ i ä¸ªæ ·æœ¬ï¼‰
- æ³¨æ„åŠ›æ©ç ï¼ˆå–å¯¹è§’çº¿æˆ–å› æœæ©ç ï¼‰

---

## è‡ªåŠ¨èµ„æºç®¡ç†ï¼šRAII ä¸ Drop Trait

### æ ¸å¿ƒæ¦‚å¿µ

RAII æ˜¯ C++ å’Œ Rust çš„æ ¸å¿ƒè®¾è®¡æ¨¡å¼ï¼š
- **è·å–èµ„æº = åˆå§‹åŒ–å¯¹è±¡**ï¼ˆå¦‚ `Buffer::new` åˆ†é…å†…å­˜ï¼‰
- **é‡Šæ”¾èµ„æº = é”€æ¯å¯¹è±¡**ï¼ˆå¦‚ `Drop::drop` é‡Šæ”¾å†…å­˜ï¼‰

åœ¨ RustInfer ä¸­ï¼Œ**æ‰€æœ‰èµ„æºéƒ½è‡ªåŠ¨ç®¡ç†ï¼Œä¸éœ€è¦æ‰‹åŠ¨æ¸…ç†**ã€‚

### Buffer çš„ RAII å®ç°

#### å†…éƒ¨ç»“æ„

```rust
// çœŸæ­£æ‹¥æœ‰å†…å­˜çš„ç»“æ„ä½“
struct BufferInner {
    ptr: NonNull<u8>,              // å†…å­˜åœ°å€
    len_bytes: usize,              // å†…å­˜å¤§å°
    allocator: Arc<dyn DeviceAllocator>, // åˆ†é…å™¨ï¼ˆCPU/CUDAï¼‰
}

// ç”¨æˆ·æŒæœ‰çš„å¥æŸ„ï¼ˆå¯ä»¥æ˜¯è§†å›¾ï¼‰
pub struct Buffer {
    inner: Option<Arc<BufferInner>>,  // Some = æ‹¥æœ‰å†…å­˜ï¼ŒNone = å¤–éƒ¨å†…å­˜
    ptr: NonNull<u8>,                 // è§†å›¾æŒ‡é’ˆï¼ˆå¯èƒ½æŒ‡å‘ inner çš„å­åŒºåŸŸï¼‰
    len_bytes: usize,                 // è§†å›¾å¤§å°
    device: DeviceType,               // è®¾å¤‡ç±»å‹
}
```

#### è‡ªåŠ¨é‡Šæ”¾æœºåˆ¶

```rust
impl Drop for BufferInner {
    fn drop(&mut self) {
        if self.len_bytes > 0 {
            let layout = Layout::from_size_align(self.len_bytes, 16).unwrap();
            unsafe {
                // è°ƒç”¨å¯¹åº”è®¾å¤‡çš„é‡Šæ”¾æ–¹æ³•
                self.allocator.deallocate(self.ptr, layout);
            }
        }
    }
}
```

**å·¥ä½œæµç¨‹**ï¼š
1. ç”¨æˆ·åˆ›å»º `Buffer::new(...)` â†’ åˆ†é…å†…å­˜ï¼Œåˆ›å»º `Arc<BufferInner>`
2. ç”¨æˆ·å…‹éš† `buffer.clone()` â†’ `Arc` å¼•ç”¨è®¡æ•° +1
3. ç”¨æˆ·åˆ›å»ºè§†å›¾ `buffer.slice(...)` â†’ `Arc` å¼•ç”¨è®¡æ•° +1ï¼Œä½†è¿”å›æ–°çš„ `ptr` å’Œ `len_bytes`
4. å½“æœ€åä¸€ä¸ª `Arc` è¢«é”€æ¯ â†’ è°ƒç”¨ `BufferInner::drop` â†’ è‡ªåŠ¨è°ƒç”¨ `cudaFree` æˆ– `free`

**ä¸ºä»€ä¹ˆä½¿ç”¨ `Option<Arc<BufferInner>>`**ï¼Ÿ

æ”¯æŒ**å¤–éƒ¨å†…å­˜**ï¼ˆå¦‚ mmap çš„ safetensors æ–‡ä»¶ï¼‰ï¼š

```rust
pub unsafe fn from_external_slice<T>(data: &[T]) -> Self {
    Buffer {
        inner: None,  // æ²¡æœ‰æ‰€æœ‰æƒï¼Œä¸ä¼šé‡Šæ”¾
        ptr: NonNull::new(data.as_ptr() as *mut u8).unwrap(),
        len_bytes: std::mem::size_of_val(data),
        device: DeviceType::Cpu,
    }
}
```

å½“ `inner = None` æ—¶ï¼Œ`Drop` ä»€ä¹ˆéƒ½ä¸åšï¼ˆå› ä¸ºæ²¡æœ‰ `BufferInner` éœ€è¦é”€æ¯ï¼‰ã€‚

### CUDA èµ„æºçš„ RAIIï¼šCudaConfig

```rust
pub struct CudaConfig {
    pub stream: cudaStream_t,            // CUDA æµ
    pub cublaslt_handle: cublasLtHandle_t, // cuBLAS å¥æŸ„
    pub workspace: *mut c_void,          // å·¥ä½œç©ºé—´å†…å­˜
    // ...
}

impl Drop for CudaConfig {
    fn drop(&mut self) {
        unsafe {
            let _ = cudaStreamDestroy(self.stream);
            let _ = cublasLtDestroy(self.cublaslt_handle);
            let _ = cudaFree(self.workspace);
        }
    }
}
```

**ä¼˜åŠ¿**ï¼š
- å³ä½¿æ¨ç†è¿‡ç¨‹ä¸­å‘ç”Ÿ panicï¼ŒCUDA èµ„æºä¹Ÿä¼šè‡ªåŠ¨æ¸…ç†
- ä¸éœ€è¦åœ¨æ¯ä¸ªå‡½æ•°ä¸­ `defer cleanup()`ï¼ˆå¦‚ Goï¼‰æˆ– `finally`ï¼ˆå¦‚ Javaï¼‰
- ç¼–è¯‘å™¨ä¿è¯ `Drop` ä¼šè¢«è°ƒç”¨ï¼ˆé™¤é `std::mem::forget`ï¼Œä½†è¿™æ˜¯ unsafe çš„ï¼‰

### RAII vs æ‰‹åŠ¨ç®¡ç†å¯¹æ¯”

**C++ æ‰‹åŠ¨ç®¡ç†**ï¼ˆå®¹æ˜“å‡ºé”™ï¼‰ï¼š
```cpp
float* buffer = nullptr;
cudaMalloc(&buffer, size);

// ... å¤æ‚é€»è¾‘ ...

if (error_condition) {
    // å®¹æ˜“å¿˜è®°é‡Šæ”¾ï¼
    return -1;  // å†…å­˜æ³„æ¼
}

cudaFree(buffer);  // åªæœ‰æ­£å¸¸è·¯å¾„ä¼šæ‰§è¡Œ
```

**RustInfer RAII**ï¼ˆè‡ªåŠ¨æ­£ç¡®ï¼‰ï¼š
```rust
let buffer = Buffer::new(size, allocator)?;

// ... å¤æ‚é€»è¾‘ ...

if error_condition {
    return Err(...);  // buffer è‡ªåŠ¨é‡Šæ”¾
}

// å‡½æ•°ç»“æŸæ—¶ buffer è‡ªåŠ¨é‡Šæ”¾
```

---

## Op Traitï¼šç»Ÿä¸€çš„ç®—å­æ¥å£

### è®¾è®¡ç›®æ ‡

1. **ç»Ÿä¸€æ¥å£**ï¼šæ‰€æœ‰ç®—å­ï¼ˆRMSNormã€Matmulã€FlashAttnï¼‰ä½¿ç”¨ç›¸åŒçš„ API
2. **è®¾å¤‡æ— å…³**ï¼šè°ƒç”¨è€…ä¸éœ€è¦çŸ¥é“ç®—å­åœ¨ CPU è¿˜æ˜¯ GPU ä¸Šæ‰§è¡Œ
3. **è¾“å…¥è¾“å‡ºçµæ´»**ï¼šæ”¯æŒå¤šè¾“å…¥å¤šè¾“å‡ºï¼ˆå¦‚ attention éœ€è¦ Qã€Kã€Vï¼‰

### Trait å®šä¹‰

```rust
pub struct OpContext<'a> {
    pub inputs: &'a [&'a Tensor],         // è¾“å…¥ tensorsï¼ˆåªè¯»ï¼‰
    pub outputs: &'a mut [&'a mut Tensor],// è¾“å‡º tensorsï¼ˆå¯å†™ï¼‰
    pub cuda_config: Option<&'a CudaConfig>, // CUDA ä¸Šä¸‹æ–‡ï¼ˆå¯é€‰ï¼‰
}

pub trait Op {
    fn name(&self) -> &'static str;
    fn forward(&self, ctx: &mut OpContext) -> Result<()>;
}
```

**ä¸ºä»€ä¹ˆä½¿ç”¨åˆ‡ç‰‡è€Œä¸æ˜¯å›ºå®šæ•°é‡å‚æ•°**ï¼Ÿ

ä¸åŒç®—å­éœ€è¦ä¸åŒæ•°é‡çš„è¾“å…¥ï¼š
- RMSNorm: 1 è¾“å…¥ï¼ˆxï¼‰+ 1 æƒé‡ â†’ 1 è¾“å‡º
- Matmul: 2 è¾“å…¥ï¼ˆA, Bï¼‰ â†’ 1 è¾“å‡º
- FlashAttn: 3 è¾“å…¥ï¼ˆQ, K, Vï¼‰ â†’ 1 è¾“å‡º + 2 ä¸­é—´ç»“æœï¼ˆå¯é€‰ï¼‰

åˆ‡ç‰‡æä¾›äº†çµæ´»æ€§ï¼ŒåŒæ—¶é€šè¿‡è¿è¡Œæ—¶æ£€æŸ¥ä¿è¯æ­£ç¡®æ€§ã€‚

### å®æˆ˜æ¡ˆä¾‹ï¼šRMSNorm ç®—å­

```rust
pub struct RMSNorm {
    pub weight: Tensor,  // ç®—å­æ‹¥æœ‰è‡ªå·±çš„æƒé‡
    dim: usize,
}

impl Op for RMSNorm {
    fn name(&self) -> &'static str { "RMSNorm" }

    fn forward(&self, ctx: &mut OpContext) -> Result<()> {
        // 1. éªŒè¯è¾“å…¥è¾“å‡º
        if ctx.inputs.len() != 1 || ctx.outputs.len() != 1 {
            return Err(Error::InvalidArgument("RMSNorm expects 1 input and 1 output".into()));
        }

        let input = ctx.inputs[0];
        let output = ctx.outputs[0];

        // 2. æ£€æŸ¥å½¢çŠ¶åŒ¹é…
        if input.shape() != output.shape() {
            return Err(Error::ShapeMismatch { ... });
        }

        // 3. è®¾å¤‡åˆ†å‘
        match input.device() {
            DeviceType::Cpu => {
                kernels::cpu::rmsnorm(input, &self.weight, output)
            }
            DeviceType::Cuda(_) => {
                kernels::cuda::rmsnorm(input, &self.weight, output, ctx.cuda_config)
            }
        }
    }
}
```

### ä¸ºä»€ä¹ˆç®—å­æŒæœ‰æƒé‡ï¼Ÿ

**ä¼˜åŠ¿**ï¼š
1. **å°è£…**ï¼šæƒé‡å’Œç®—å­é€»è¾‘ç»‘å®šï¼Œä¸ä¼šä¼ é”™
2. **ç±»å‹å®‰å…¨**ï¼š`weight` çš„ç±»å‹åœ¨ç¼–è¯‘æ—¶ç¡®å®š
3. **æ˜“äºç§»åŠ¨åˆ° GPU**ï¼š`rmsnorm.to_cuda()` ä¼šç§»åŠ¨æƒé‡å’Œç®—å­

**ä¸ PyTorch å¯¹æ¯”**ï¼š
```python
# PyTorch: æƒé‡å’Œç®—å­åˆ†ç¦»
class LlamaModel(nn.Module):
    def __init__(self):
        self.norm_weight = nn.Parameter(...)

    def forward(self, x):
        return F.rms_norm(x, self.norm_weight)  # å®¹æ˜“ä¼ é”™å‚æ•°

# RustInfer: æƒé‡å’Œç®—å­ç»‘å®š
let rmsnorm = RMSNorm { weight: load_weight("norm.weight"), dim: 4096 };
rmsnorm.forward(&mut ctx)?;  // ä¸å¯èƒ½ä¼ é”™æƒé‡
```

### è®¾å¤‡åˆ†å‘æ¨¡å¼

æ¯ä¸ªç®—å­å†…éƒ¨å†³å®šä½¿ç”¨å“ªä¸ª kernelï¼š

```rust
match input.device() {
    DeviceType::Cpu => {
        // è°ƒç”¨ CPU kernelï¼ˆçº¯ Rust æˆ–é€šè¿‡ BLASï¼‰
        cpu_kernel(...)
    }
    DeviceType::Cuda(device_id) => {
        // è°ƒç”¨ CUDA kernelï¼ˆextern "C" FFIï¼‰
        unsafe {
            cuda_kernel_launch(..., ctx.cuda_config.unwrap().stream);
        }
        cuda_check!(cudaGetLastError())?;
    }
}
```

**ä¸ºä»€ä¹ˆä¸ç”¨ trait object åˆ†å‘**ï¼Ÿ

```rust
// ä¸å¥½çš„è®¾è®¡ï¼š
trait CpuOp { fn forward_cpu(...); }
trait CudaOp { fn forward_cuda(...); }

// é—®é¢˜ï¼š
// 1. éœ€è¦ä¸¤ä¸ª traitï¼Œå¢åŠ å¤æ‚åº¦
// 2. æ— æ³•åœ¨ç¼–è¯‘æ—¶ç¡®å®šè®¾å¤‡
// 3. åŠ¨æ€åˆ†å‘æœ‰æ€§èƒ½å¼€é”€
```

å½“å‰è®¾è®¡çš„ `match` è¯­å¥ä¼šè¢«ç¼–è¯‘å™¨ä¼˜åŒ–ä¸ºç›´æ¥è·³è½¬ï¼ˆé›¶å¼€é”€ï¼‰ã€‚

---

## Model Traitï¼šæ¨¡å‹æŠ½è±¡å±‚

### è®¾è®¡ç›®æ ‡

æä¾›ç»Ÿä¸€æ¥å£ï¼Œä½¿å¾—ä¸åŒæ¨¡å‹ï¼ˆLlama3ã€GPTã€BERTï¼‰å¯ä»¥äº’æ¢ï¼š

```rust
pub trait Model {
    fn init(&mut self, device_type: DeviceType) -> Result<()>;
    fn forward(&mut self, input: &Tensor, pos: &Tensor) -> Result<Tensor>;
    fn tokenizer(&self) -> &dyn Tokenizer;
    fn is_eos_token(&self, token_id: u32) -> bool;
    fn slice_kv_cache(&self, layer: usize, start: usize, end: usize) -> Result<(Tensor, Tensor)>;
}
```

### Llama3 å®ç°

```rust
pub struct Llama3 {
    config: RuntimeModelConfig,
    device_type: DeviceType,
    tokenizer: Box<dyn Tokenizer>,  // Trait objectï¼Œæ”¯æŒä¸åŒ tokenizer
    layers: LlamaLayers,            // æ‰€æœ‰ç®—å­
    kv_cache: KvCache,              // KV cache
    workspace: Workspace,           // é¢„åˆ†é…çš„ä¸­é—´ buffer
    sampler: Box<dyn Sampler>,      // é‡‡æ ·å™¨
    cuda_config: Option<CudaConfig>,// CUDA ä¸Šä¸‹æ–‡
}

pub struct LlamaLayers {
    pub embedding_layer: Embedding,
    pub wq_layers: Vec<Matmul>,      // æ¯ä¸€å±‚çš„ Q æŠ•å½±
    pub wk_layers: Vec<Matmul>,      // æ¯ä¸€å±‚çš„ K æŠ•å½±
    pub mha_layers: Vec<FlashAttnGQA>,
    // ... å…± 28 ä¸ªç®—å­é›†åˆ
}
```

### ä¸ºä»€ä¹ˆä½¿ç”¨ Vec<Op> è€Œä¸æ˜¯å¾ªç¯è°ƒç”¨å‡½æ•°ï¼Ÿ

**ç®—å­ä¸ºä¸­å¿ƒçš„è®¾è®¡**ï¼š

```rust
// ä¸å¥½çš„è®¾è®¡ï¼ˆå‡½æ•°ä¸ºä¸­å¿ƒï¼‰ï¼š
impl Llama3 {
    fn attention_layer_0(&mut self, x: Tensor) -> Tensor { ... }
    fn attention_layer_1(&mut self, x: Tensor) -> Tensor { ... }
    // ... é‡å¤ 32 æ¬¡
}

// å¥½çš„è®¾è®¡ï¼ˆç®—å­ä¸ºä¸­å¿ƒï¼‰ï¼š
impl Llama3 {
    fn forward(&mut self, x: Tensor) -> Tensor {
        for layer_idx in 0..self.config.n_layers {
            x = self.wq_layers[layer_idx].forward(x)?;
            // ...
        }
    }
}
```

**ä¼˜åŠ¿**ï¼š
1. **æ•°æ®é©±åŠ¨**ï¼šå±‚æ•°ç”±é…ç½®å†³å®šï¼Œä¸æ˜¯ç¡¬ç¼–ç 
2. **æ˜“äºå¹¶è¡Œ**ï¼š`layers.par_iter()` å¯ä»¥å®ç°æµæ°´çº¿å¹¶è¡Œ
3. **åŠ¨æ€ä¼˜åŒ–**ï¼šå¯ä»¥åœ¨è¿è¡Œæ—¶é‡æ’ç®—å­é¡ºåºã€èåˆç®—å­

### é¢„åˆ†é… Workspace æ¨¡å¼

```rust
pub type Workspace = HashMap<BufferType, Tensor>;

fn init_workspace(config: &RuntimeModelConfig, device: DeviceType) -> Result<Workspace> {
    let mut buffers = HashMap::new();

    // é¢„åˆ†é…æ‰€æœ‰ä¸­é—´ buffer
    buffers.insert(BufferType::Query, Tensor::new(&[seq_len, dim], dtype, device)?);
    buffers.insert(BufferType::Key, Tensor::new(&[seq_len, kv_dim], dtype, device)?);
    buffers.insert(BufferType::W1Output, Tensor::new(&[seq_len, inter_dim], dtype, device)?);
    // ... 15+ ä¸ª buffer

    Ok(buffers)
}

impl Llama3 {
    fn forward(&mut self, input: &Tensor) -> Result<Tensor> {
        let query_buf = self.workspace.get_mut(&BufferType::Query).unwrap();
        self.wq_layers[0].forward_into(input, query_buf)?;  // ç›´æ¥å†™å…¥é¢„åˆ†é…çš„ buffer
        // ...
    }
}
```

**ä¸ºä»€ä¹ˆé‡è¦**ï¼š
- **æ¶ˆé™¤åˆ†é…**ï¼šæ¨ç†å¾ªç¯ä¸­æ²¡æœ‰ `malloc`/`cudaMalloc` è°ƒç”¨
- **å¯é¢„æµ‹å†…å­˜**ï¼šæ€»å†…å­˜ä½¿ç”¨åœ¨æ¨¡å‹åŠ è½½æ—¶å°±ç¡®å®š
- **ç¼“å­˜å‹å¥½**ï¼šé‡å¤ä½¿ç”¨ç›¸åŒ bufferï¼Œæé«˜ L2 ç¼“å­˜å‘½ä¸­ç‡

**æ€§èƒ½æ•°æ®**ï¼ˆ7B æ¨¡å‹ï¼Œå•ä¸ª tokenï¼‰ï¼š
```
æœ‰ workspace:    ~50Âµsï¼ˆçº¯è®¡ç®—ï¼‰
æ—  workspace:    ~150Âµsï¼ˆåˆ†é… + è®¡ç®— + é‡Šæ”¾ï¼‰
```

---

## è®¾å¤‡æŠ½è±¡ï¼šCPU ä¸ CUDA ç»Ÿä¸€æ¥å£

### DeviceType æšä¸¾

```rust
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum DeviceType {
    Cpu,
    #[cfg(feature = "cuda")]
    Cuda(i32),  // è®¾å¤‡ ID åµŒå…¥ç±»å‹ä¸­
}
```

**ä¸ºä»€ä¹ˆè®¾å¤‡ ID æ˜¯æšä¸¾çš„ä¸€éƒ¨åˆ†**ï¼Ÿ

1. **ç±»å‹å®‰å…¨**ï¼šä¸èƒ½å°† GPU 0 çš„ tensor å’Œ GPU 1 çš„ tensor æ··ç”¨
2. **æ˜¾å¼æ€§**ï¼šä¸€çœ¼çœ‹å‡º tensor åœ¨å“ªä¸ªè®¾å¤‡ä¸Š
3. **é›¶å¼€é”€**ï¼š`DeviceType` æ˜¯ `Copy` çš„ï¼Œä¼ é€’åªéœ€ 8 å­—èŠ‚

### DeviceAllocator Trait

```rust
pub trait DeviceAllocator {
    fn device(&self) -> DeviceType;
    unsafe fn allocate(&self, layout: Layout) -> Result<NonNull<u8>>;
    unsafe fn deallocate(&self, ptr: NonNull<u8>, layout: Layout);
}

// CPU å®ç°
pub struct CpuAllocator;
impl DeviceAllocator for CpuAllocator {
    fn device(&self) -> DeviceType { DeviceType::Cpu }
    unsafe fn allocate(&self, layout: Layout) -> Result<NonNull<u8>> {
        Ok(NonNull::new(std::alloc::alloc(layout)).unwrap())
    }
    unsafe fn deallocate(&self, ptr: NonNull<u8>, layout: Layout) {
        std::alloc::dealloc(ptr.as_ptr(), layout);
    }
}

// CUDA å®ç°
pub struct CachingCudaAllocator { ... }
impl DeviceAllocator for CachingCudaAllocator {
    fn device(&self) -> DeviceType { DeviceType::Cuda(/* å½“å‰è®¾å¤‡ */) }
    unsafe fn allocate(&self, layout: Layout) -> Result<NonNull<u8>> {
        // ä»å†…å­˜æ± è·å–æˆ–è°ƒç”¨ cudaMalloc
    }
    unsafe fn deallocate(&self, ptr: NonNull<u8>, layout: Layout) {
        // å½’è¿˜åˆ°å†…å­˜æ± æˆ–è°ƒç”¨ cudaFree
    }
}
```

**ä¼˜åŠ¿**ï¼š
- æ·»åŠ æ–°è®¾å¤‡ï¼ˆMetalã€Vulkanï¼‰åªéœ€å®ç°è¿™ä¸ª trait
- `Buffer` ä¸éœ€è¦çŸ¥é“åº•å±‚æ˜¯ CPU è¿˜æ˜¯ GPU
- å¯ä»¥åœ¨è¿è¡Œæ—¶åˆ‡æ¢åˆ†é…å™¨ï¼ˆå¦‚å¯ç”¨/ç¦ç”¨å†…å­˜æ± ï¼‰

### CUDA é”™è¯¯å¤„ç†å®

```rust
#[macro_export]
macro_rules! cuda_check {
    ($expr:expr) => {
        {
            let result = $expr;
            if result != cudaError_cudaSuccess {
                return Err(Error::CudaError(CudaError(result)));
            }
        }
    };
}

// ä½¿ç”¨ç¤ºä¾‹
cuda_check!(cudaMalloc(&mut ptr, size));
cuda_check!(cudaMemcpy(dst, src, size, cudaMemcpyDeviceToHost));
```

**ä¸ºä»€ä¹ˆç”¨å®è€Œä¸æ˜¯å‡½æ•°**ï¼Ÿ
1. **é›¶å¼€é”€**ï¼šå®åœ¨ç¼–è¯‘æ—¶å±•å¼€ï¼Œæ²¡æœ‰å‡½æ•°è°ƒç”¨
2. **ä¿ç•™è¡Œå·**ï¼šé”™è¯¯ä¼šæŒ‡å‘å®é™…çš„ FFI è°ƒç”¨ä½ç½®
3. **ç±»å‹çµæ´»**ï¼šå¯ä»¥å¤„ç†ä¸åŒè¿”å›ç±»å‹çš„ CUDA å‡½æ•°

---

## å†…å­˜æ± åŒ–ï¼šCachingCudaAllocator

### ä¸ºä»€ä¹ˆéœ€è¦å†…å­˜æ± ï¼Ÿ

**é—®é¢˜**ï¼š`cudaMalloc` å’Œ `cudaFree` å¾ˆæ…¢ï¼š
```
cudaMalloc(1MB):  ~800Âµsï¼ˆéœ€è¦å’Œ GPU é©±åŠ¨é€šä¿¡ï¼‰
malloc(1MB):      ~5Âµsï¼ˆåªéœ€è¦ç³»ç»Ÿè°ƒç”¨ï¼‰
```

æ¨ç†è¿‡ç¨‹ä¸­é¢‘ç¹åˆ†é…ä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™ã€‚

### å†…å­˜æ± è®¾è®¡

```rust
pub struct CachingCudaAllocator {
    state: AllocatorState,
}

struct AllocatorState {
    // å°å—å†…å­˜æ± ï¼ˆ<1MBï¼‰ï¼Œä½¿ç”¨é¦–æ¬¡é€‚é…ç­–ç•¥
    small_pool: DashMap<i32, Vec<CudaMemoryChunk>>,  // key = device_id

    // å¤§å—å†…å­˜æ± ï¼ˆ>=1MBï¼‰ï¼Œä½¿ç”¨æœ€ä½³é€‚é…ç­–ç•¥
    large_pool: DashMap<i32, Vec<CudaMemoryChunk>>,

    // æ¯ä¸ªè®¾å¤‡çš„ç©ºé—²å†…å­˜ç»Ÿè®¡
    idle_bytes: DashMap<i32, usize>,
}

struct CudaMemoryChunk {
    ptr: NonNull<u8>,
    size: usize,
    stream: cudaStream_t,  // åˆ†é…æ—¶çš„æµï¼ˆç¡®ä¿åŒæ­¥ï¼‰
}
```

### åˆ†é…ç­–ç•¥

```rust
impl CachingCudaAllocator {
    unsafe fn allocate(&self, layout: Layout) -> Result<NonNull<u8>> {
        let size = layout.size();
        let device_id = get_current_device()?;

        // 1. ä»å¯¹åº”çš„æ± ä¸­æŸ¥æ‰¾
        let pool = if size < 1MB { &self.small_pool } else { &self.large_pool };

        if let Some(chunk) = pool.find_suitable(device_id, size) {
            // 2. æ‰¾åˆ°åˆé€‚çš„å—ï¼Œç›´æ¥è¿”å›
            return Ok(chunk.ptr);
        }

        // 3. æ± ä¸­æ²¡æœ‰ï¼Œè°ƒç”¨ cudaMalloc
        let mut ptr = std::ptr::null_mut();
        cuda_check!(cudaMalloc(&mut ptr, size))?;

        Ok(NonNull::new(ptr).unwrap())
    }

    unsafe fn deallocate(&self, ptr: NonNull<u8>, layout: Layout) {
        let size = layout.size();
        let device_id = get_current_device().unwrap();

        // 1. å½’è¿˜åˆ°æ± ä¸­
        let pool = if size < 1MB { &self.small_pool } else { &self.large_pool };
        pool.push(CudaMemoryChunk { ptr, size, ... });

        // 2. æ›´æ–°ç©ºé—²å†…å­˜è®¡æ•°
        *self.idle_bytes.entry(device_id).or_insert(0) += size;

        // 3. åƒåœ¾å›æ”¶ï¼šå¦‚æœç©ºé—²å†…å­˜è¶…è¿‡ 1GBï¼ŒçœŸæ­£é‡Šæ”¾ä¸€äº›å—
        if self.idle_bytes[&device_id] > 1GB {
            self.garbage_collect(device_id);
        }
    }
}
```

### ä¸ºä»€ä¹ˆåˆ†å¤§å°æ± ï¼Ÿ

**é¦–æ¬¡é€‚é… vs æœ€ä½³é€‚é…**ï¼š

- **é¦–æ¬¡é€‚é…ï¼ˆFirst-fitï¼‰**ï¼šæ‰¾åˆ°ç¬¬ä¸€ä¸ªè¶³å¤Ÿå¤§çš„å—å°±è¿”å›
  - ä¼˜ç‚¹ï¼šå¿«ï¼ˆO(1) å¹³å‡ï¼‰
  - ç¼ºç‚¹ï¼šå¯èƒ½é€ æˆç¢ç‰‡
  - é€‚ç”¨åœºæ™¯ï¼šå°å—å†…å­˜ï¼ˆ<1MBï¼‰ï¼Œç¢ç‰‡å½±å“å°

- **æœ€ä½³é€‚é…ï¼ˆBest-fitï¼‰**ï¼šæ‰¾åˆ°æœ€æ¥è¿‘è¯·æ±‚å¤§å°çš„å—
  - ä¼˜ç‚¹ï¼šå‡å°‘ç¢ç‰‡
  - ç¼ºç‚¹ï¼šæ…¢ï¼ˆO(n)ï¼‰
  - é€‚ç”¨åœºæ™¯ï¼šå¤§å—å†…å­˜ï¼ˆ>=1MBï¼‰ï¼Œç¢ç‰‡å½±å“å¤§

### æ€§èƒ½æå‡

**å®æµ‹æ•°æ®**ï¼ˆLlama-3-8Bï¼Œbatch=1ï¼‰ï¼š
```
æ— å†…å­˜æ± ï¼š    120 tokens/sï¼ˆæ¯ä¸ª token ~800Âµs ç”¨äºåˆ†é…ï¼‰
æœ‰å†…å­˜æ± ï¼š    180 tokens/sï¼ˆé¦–æ¬¡åˆ†é…åï¼Œåç»­ ~1Âµsï¼‰
æå‡ï¼š       50%
```

---

## KV Cache ç®¡ç†ï¼šé›¶æ‹·è´è§†å›¾

### ä»€ä¹ˆæ˜¯ KV Cacheï¼Ÿ

åœ¨è‡ªå›å½’ç”Ÿæˆä¸­ï¼Œæ¯ä¸ª token çš„ Key å’Œ Value ä¼šè¢«é‡å¤ä½¿ç”¨ï¼š

```
Token 0: Q0 @ [K0, V0] â†’ Output0
Token 1: Q1 @ [K0, K1, V0, V1] â†’ Output1  ï¼ˆK0, V0 è¢«é‡ç”¨ï¼‰
Token 2: Q2 @ [K0, K1, K2, V0, V1, V2] â†’ Output2  ï¼ˆK0, K1, V0, V1 è¢«é‡ç”¨ï¼‰
```

å¦‚æœæ¯æ¬¡éƒ½é‡æ–°è®¡ç®—æ‰€æœ‰ K/Vï¼Œæ—¶é—´å¤æ‚åº¦æ˜¯ O(nÂ²)ã€‚KV Cache å°†æ—¶é—´å¤æ‚åº¦é™ä½åˆ° O(n)ã€‚

### RustInfer çš„ KV Cache è®¾è®¡

```rust
struct KvCache {
    // æ¯ä¸€å±‚å­˜å‚¨ä¸€å¯¹ (K, V) tensor
    cache: Vec<(Tensor, Tensor)>,
}

impl KvCache {
    fn new(config: &RuntimeModelConfig, device: DeviceType) -> Result<Self> {
        let mut cache = Vec::new();

        for _ in 0..config.n_layers {
            // é¢„åˆ†é…æœ€å¤§é•¿åº¦çš„ cache
            let k = Tensor::new(
                &[config.max_seq_len, config.kv_dim],  // å½¢çŠ¶ï¼š[max_len, kv_dim]
                config.dtype,
                device
            )?;
            let v = k.clone();  // V å½¢çŠ¶ç›¸åŒ
            cache.push((k, v));
        }

        Ok(KvCache { cache })
    }

    fn slice_kv_cache(
        &mut self,
        layer_idx: usize,
        start_pos: usize,
        len: usize,
    ) -> Result<(Tensor, Tensor)> {
        let (k_full, v_full) = &mut self.cache[layer_idx];

        // é›¶æ‹·è´åˆ‡ç‰‡ï¼šåªåˆ›å»ºè§†å›¾ï¼Œä¸æ‹·è´æ•°æ®
        let k_slice = k_full.slice(&[start_pos, 0], &[len, self.kv_dim])?;
        let v_slice = v_full.slice(&[start_pos, 0], &[len, self.kv_dim])?;

        Ok((k_slice, v_slice))
    }
}
```

### é›¶æ‹·è´çš„å·¥ä½œåŸç†

```rust
// 1. è®¡ç®—æ–° K/V
let new_k = self.wk_layers[layer].forward(x)?;  // å½¢çŠ¶ï¼š[1, kv_dim]

// 2. è·å–å½“å‰ä½ç½®çš„ cache åˆ‡ç‰‡
let (k_cache_slot, _) = self.kv_cache.slice_kv_cache(layer, pos, 1)?;

// 3. ç›´æ¥å†™å…¥ cacheï¼ˆè¦†ç›– cache çš„å†…å­˜ï¼‰
k_cache_slot.copy_from(&new_k)?;

// 4. è·å–æ‰€æœ‰å†å² K/Vï¼ˆç”¨äº attentionï¼‰
let (k_all, v_all) = self.kv_cache.slice_kv_cache(layer, 0, pos + 1)?;

// 5. è®¡ç®— attention
let attn_out = self.mha_layers[layer].forward(&q, &k_all, &v_all)?;
```

**å…³é”®ç‚¹**ï¼š
- `slice_kv_cache` è¿”å›çš„æ˜¯è§†å›¾ï¼ŒæŒ‡å‘åŸå§‹ cache çš„å†…å­˜
- `copy_from` ç›´æ¥å†™å…¥ cacheï¼Œä¸éœ€è¦é¢å¤–çš„"å†™å›"æ“ä½œ
- æ‰€æœ‰åˆ‡ç‰‡å…±äº«åŒä¸€å— GPU å†…å­˜ï¼Œå®Œå…¨é›¶æ‹·è´

### å†…å­˜å¸ƒå±€å¯è§†åŒ–

```
åŸå§‹ cache: [max_seq_len=2048, kv_dim=128]  ï¼ˆGPU å†…å­˜ï¼‰
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”
â”‚ K0  â”‚ K1  â”‚ K2  â”‚ ... â”‚     â”‚     â”‚     â”‚  â† å·²ä½¿ç”¨çš„éƒ¨åˆ†
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜
  â†‘     â†‘     â†‘
  â”‚     â”‚     â””â”€ slice(&[2, 0], &[1, 128])  â† å†™å…¥ä½ç½®
  â”‚     â””â”€ slice(&[1, 0], &[1, 128])
  â””â”€ slice(&[0, 0], &[3, 128])  â† è¯»å–æ‰€æœ‰å†å²
```

---

## Workspace æ¨¡å¼ï¼šé¢„åˆ†é…å†…å­˜

### é—®é¢˜èƒŒæ™¯

æ¨ç†è¿‡ç¨‹éœ€è¦å¤§é‡ä¸´æ—¶ bufferï¼š

```rust
// æ¯ä¸ª token éƒ½éœ€è¦è¿™äº› buffer
let q = Tensor::new(&[seq_len, dim], dtype, device)?;  // Query
let k = Tensor::new(&[seq_len, kv_dim], dtype, device)?;  // Key
let v = Tensor::new(&[seq_len, kv_dim], dtype, device)?;  // Value
let attn_out = Tensor::new(&[seq_len, dim], dtype, device)?;  // Attention output
let ffn_intermediate = Tensor::new(&[seq_len, 4*dim], dtype, device)?;  // FFN intermediate
// ... 10+ ä¸ª buffer
```

å¦‚æœæ¯æ¬¡éƒ½åˆ†é…ï¼Œä¼šå¯¼è‡´æ€§èƒ½ä¸‹é™å’Œå†…å­˜ç¢ç‰‡ã€‚

### Workspace è®¾è®¡

```rust
pub type Workspace = HashMap<BufferType, Tensor>;

#[derive(Debug, Hash, Eq, PartialEq)]
pub enum BufferType {
    Query,
    Key,
    Value,
    AttnOutput,
    W1Output,  // FFN gate
    W2Output,  // FFN down
    W3Output,  // FFN up
    // ... 15+ ç±»å‹
}

impl Llama3 {
    fn init_workspace(config: &RuntimeModelConfig, device: DeviceType) -> Result<Workspace> {
        let mut workspace = HashMap::new();

        // é¢„åˆ†é…æ‰€æœ‰å¯èƒ½éœ€è¦çš„ buffer
        workspace.insert(
            BufferType::Query,
            Tensor::new(&[config.max_batch_size, config.dim], config.dtype, device)?
        );
        workspace.insert(
            BufferType::Key,
            Tensor::new(&[config.max_batch_size, config.kv_dim], config.dtype, device)?
        );
        // ... å…¶ä½™ buffer

        Ok(workspace)
    }

    fn forward(&mut self, x: &Tensor) -> Result<Tensor> {
        // ä» workspace è·å–é¢„åˆ†é…çš„ buffer
        let q_buf = self.workspace.get_mut(&BufferType::Query).unwrap();

        // ç›´æ¥å†™å…¥ï¼Œä¸åˆ†é…æ–°å†…å­˜
        self.wq_layers[layer].forward_into(x, q_buf)?;

        // ...
    }
}
```

### ä¼˜åŠ¿

1. **é›¶åˆ†é…**ï¼šæ¨ç†å¾ªç¯ä¸­å®Œå…¨æ²¡æœ‰å†…å­˜åˆ†é…
2. **å¯é¢„æµ‹**ï¼šæœ€å¤§å†…å­˜ä½¿ç”¨åœ¨å¯åŠ¨æ—¶å°±ç¡®å®š
3. **ç¼“å­˜å‹å¥½**ï¼šé‡å¤ä½¿ç”¨ç›¸åŒåœ°å€ï¼Œæé«˜ cache å‘½ä¸­ç‡

**æ€§èƒ½æ•°æ®**ï¼ˆå•ä¸ª token å‰å‘ä¼ æ’­ï¼‰ï¼š
```
æ—  workspace:  150Âµsï¼ˆ50Âµs åˆ†é… + 80Âµs è®¡ç®— + 20Âµs é‡Šæ”¾ï¼‰
æœ‰ workspace:  80Âµsï¼ˆçº¯è®¡ç®—ï¼‰
æå‡:         ~2x
```

---

## é›¶æ‹·è´æƒé‡åŠ è½½

### é—®é¢˜ï¼šä¼ ç»Ÿæƒé‡åŠ è½½çš„å¼€é”€

**ä¼ ç»Ÿæ–¹æ³•**ï¼ˆå¦‚ PyTorchï¼‰ï¼š
```python
# 1. è¯»å–æ–‡ä»¶åˆ°å†…å­˜
with open("model.safetensors", "rb") as f:
    data = f.read()  # æ‹·è´ 1ï¼šç£ç›˜ â†’ é¡µç¼“å­˜ â†’ ç”¨æˆ·ç©ºé—´

# 2. è§£æ safetensors æ ¼å¼
tensors = safetensors.load(data)  # æ‹·è´ 2ï¼šå­—èŠ‚ â†’ å¼ é‡

# 3. è½¬ç§»åˆ° GPU
model.load_state_dict(tensors)  # æ‹·è´ 3ï¼šCPU â†’ GPU
```

å¯¹äº 7B æ¨¡å‹ï¼ˆ14GB BF16 æƒé‡ï¼‰ï¼Œè¿™æ„å‘³ç€ï¼š
- æ‹·è´ 1ï¼š14GB
- æ‹·è´ 2ï¼š14GB
- æ‹·è´ 3ï¼š14GB
- **æ€»è®¡ï¼š42GB æ•°æ®ç§»åŠ¨**ï¼Œè€—æ—¶ ~10 ç§’

### RustInfer çš„é›¶æ‹·è´æ–¹æ¡ˆ

```rust
pub struct ModelLoader {
    config: RuntimeModelConfig,
    _mmaps: HashMap<PathBuf, Mmap>,  // æŒæœ‰ mmap å¯¹è±¡ï¼ˆä¿æŒç”Ÿå‘½å‘¨æœŸï¼‰
    readers: HashMap<PathBuf, SafetensorReader<'static>>,  // 'static ç”Ÿå‘½å‘¨æœŸçš„ reader
}

impl ModelLoader {
    pub fn new(model_path: &Path, config: RuntimeModelConfig) -> Result<Self> {
        let mut _mmaps = HashMap::new();
        let mut readers = HashMap::new();

        // 1. mmap æ‰€æœ‰ safetensors æ–‡ä»¶
        for file in glob("*.safetensors")? {
            let mmap = unsafe { Mmap::open(&file)? };  // é›¶æ‹·è´ï¼šç›´æ¥æ˜ å°„åˆ°è¿›ç¨‹åœ°å€ç©ºé—´

            // 2. å°† mmap çš„ç”Ÿå‘½å‘¨æœŸ"å»¶é•¿"åˆ° 'static
            // SAFETY: _mmaps å­—æ®µåœ¨ readers ä¹‹å‰ï¼ŒRust ä¿è¯å…ˆ drop readers å† drop _mmaps
            let mmap_static: &'static [u8] = unsafe {
                std::mem::transmute(mmap.as_ref())
            };

            let reader = SafetensorReader::new(mmap_static)?;

            _mmaps.insert(file.clone(), mmap);
            readers.insert(file, reader);
        }

        Ok(ModelLoader { config, _mmaps, readers })
    }

    pub fn load_tensor(&self, name: &str) -> Result<Tensor> {
        // 3. æŸ¥æ‰¾ tensor
        for reader in self.readers.values() {
            if let Some(view) = reader.tensor(name) {
                // 4. åˆ›å»º Bufferï¼ŒåŒ…è£… mmap çš„å†…å­˜ï¼ˆä¸æ‹¥æœ‰æ‰€æœ‰æƒï¼‰
                let buffer = unsafe {
                    Buffer::from_external_slice(view.data())
                };

                // 5. åˆ›å»º Tensorï¼ˆé›¶æ‹·è´ï¼‰
                return Tensor::from_buffer(buffer, view.shape());
            }
        }
        Err(Error::TensorNotFound(name.to_string()))
    }
}
```

### ä¸ºä»€ä¹ˆè¿™æ˜¯å®‰å…¨çš„ï¼Ÿ

**å…³é”®**ï¼šRust çš„ drop é¡ºåºä¿è¯

```rust
struct ModelLoader {
    _mmaps: HashMap<PathBuf, Mmap>,       // å­—æ®µ 1ï¼šå…ˆå£°æ˜
    readers: HashMap<PathBuf, SafetensorReader<'static>>,  // å­—æ®µ 2ï¼šåå£°æ˜
}

// Rust ä¿è¯ï¼š
// - drop é¡ºåºä¸å£°æ˜é¡ºåºç›¸å
// - å…ˆ drop readersï¼Œå† drop _mmaps
// - å› æ­¤ readers ä½¿ç”¨çš„å†…å­˜ï¼ˆmmapï¼‰åœ¨å®ƒä»¬è¢«é”€æ¯åæ‰é‡Šæ”¾
```

**æ€§èƒ½**ï¼š
```
ä¼ ç»ŸåŠ è½½ï¼š10 ç§’ï¼ˆ42GB æ‹·è´ï¼‰
é›¶æ‹·è´åŠ è½½ï¼š0.1 ç§’ï¼ˆåªè§£æå…ƒæ•°æ®ï¼‰
æå‡ï¼š     100x
```

### ç§»åŠ¨åˆ° GPU

```rust
impl Llama3 {
    pub fn init(&mut self, device: DeviceType) -> Result<()> {
        match device {
            DeviceType::Cpu => {
                // CPUï¼šæƒé‡å·²ç»åœ¨ CPU å†…å­˜ä¸­ï¼ˆmmapï¼‰ï¼Œä»€ä¹ˆéƒ½ä¸åš
            }
            DeviceType::Cuda(device_id) => {
                // GPUï¼šå°†æ‰€æœ‰å±‚çš„æƒé‡ç§»åˆ° GPU
                self.layers.to_cuda(device_id)?;  // æ‹·è´ï¼šCPU â†’ GPU
            }
        }
    }
}

impl LlamaLayers {
    fn to_cuda(&mut self, device_id: i32) -> Result<()> {
        // éå†æ‰€æœ‰å±‚ï¼Œè°ƒç”¨ to_cuda
        self.wq_layers.iter_mut().try_for_each(|layer| layer.to_cuda(device_id))?;
        self.wk_layers.iter_mut().try_for_each(|layer| layer.to_cuda(device_id))?;
        // ...
    }
}

impl Matmul {
    fn to_cuda(&mut self, device_id: i32) -> Result<()> {
        // åªæ‹·è´æƒé‡ï¼Œä¸æ‹·è´ä¸¤æ¬¡
        self.weight = self.weight.to_cuda(device_id)?;
    }
}
```

**æ€»æ‹·è´**ï¼š
- CPU æ¨ç†ï¼š0 å­—èŠ‚ï¼ˆç›´æ¥ä½¿ç”¨ mmapï¼‰
- GPU æ¨ç†ï¼š14GBï¼ˆä¸€æ¬¡ CPU â†’ GPU æ‹·è´ï¼‰

---

## æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

### 1. BF16 æ¨ç†ï¼ˆGPUï¼‰

**BF16 (BFloat16)** æ˜¯ Google ä¸ºæ·±åº¦å­¦ä¹ è®¾è®¡çš„ 16 ä½æµ®ç‚¹æ ¼å¼ï¼š

```
FP32:  1 sign + 8 exponent + 23 mantissa = 32 bits
BF16:  1 sign + 8 exponent + 7 mantissa  = 16 bits
FP16:  1 sign + 5 exponent + 10 mantissa = 16 bits
```

**ä¸ºä»€ä¹ˆé€‰æ‹© BF16 è€Œä¸æ˜¯ FP16ï¼Ÿ**
- BF16 å’Œ FP32 çš„æŒ‡æ•°èŒƒå›´ç›¸åŒï¼ˆ-126 åˆ° 127ï¼‰ï¼Œä¸å®¹æ˜“æº¢å‡º
- FP16 æŒ‡æ•°èŒƒå›´å°ï¼ˆ-14 åˆ° 15ï¼‰ï¼ŒLLM æ¨ç†ä¸­å®¹æ˜“æº¢å‡º
- Ampere æ¶æ„å¼€å§‹ï¼ŒBF16 å’Œ FP16 æ€§èƒ½ç›¸åŒ

**RustInfer çš„ BF16 ç­–ç•¥**ï¼š
```rust
// æƒé‡åŠ è½½æ—¶è½¬æ¢ä¸º BF16
let weight = loader.load_tensor("wq.weight")?;
let weight_bf16 = weight.to_dtype(DataType::BF16)?;

// æ‰€æœ‰ä¸­é—´æ¿€æ´»éƒ½æ˜¯ BF16
let q = Tensor::new(&[seq_len, dim], DataType::BF16, device)?;

// åªæœ‰ logits ä½¿ç”¨ FP32ï¼ˆä¸ºäº†æ•°å€¼ç¨³å®šæ€§ï¼‰
let logits = logits_bf16.to_dtype(DataType::F32)?;
```

**æ€§èƒ½æå‡**ï¼š
```
FP32 æ¨ç†ï¼š   60 tokens/sï¼ˆå†…å­˜å¸¦å®½ç“¶é¢ˆï¼‰
BF16 æ¨ç†ï¼š   120 tokens/sï¼ˆ2x å†…å­˜å¸¦å®½ï¼Œ2x ååé‡ï¼‰
ç²¾åº¦æŸå¤±ï¼š   < 0.1%ï¼ˆperplexity å‡ ä¹æ— å·®å¼‚ï¼‰
```

### 2. Fused Kernels

**Kernel èåˆ** å°†å¤šä¸ªæ“ä½œåˆå¹¶ä¸ºä¸€ä¸ª CUDA kernelï¼Œå‡å°‘å†…å­˜è®¿é—®ï¼š

#### Flash Attention

**ä¼ ç»Ÿ attentionï¼ˆ3 ä¸ª kernelï¼‰**ï¼š
```python
# Kernel 1: è®¡ç®— scores
scores = Q @ K.T  # [seq_len, seq_len]

# Kernel 2: softmax
attn_weights = softmax(scores)  # è¯»å†™ scores

# Kernel 3: è®¡ç®— output
output = attn_weights @ V  # è¯» attn_weights
```

**Flash Attentionï¼ˆ1 ä¸ª kernelï¼‰**ï¼š
```cuda
// æ‰€æœ‰æ“ä½œåœ¨åŒä¸€ä¸ª kernel ä¸­å®Œæˆï¼Œä¸­é—´ç»“æœä¸å†™å›æ˜¾å­˜
__global__ void flash_attention_kernel(...) {
    // åˆ†å—è®¡ç®—ï¼Œå…±äº«å†…å­˜å­˜å‚¨ä¸­é—´ç»“æœ
    __shared__ float scores[BLOCK_SIZE][BLOCK_SIZE];

    // 1. è®¡ç®— scores
    // 2. softmaxï¼ˆåœ¨å…±äº«å†…å­˜ä¸­ï¼‰
    // 3. è®¡ç®— output

    // åªæœ‰æœ€ç»ˆç»“æœå†™å›æ˜¾å­˜
}
```

**æ€§èƒ½æå‡**ï¼š
```
ä¼ ç»Ÿ attentionï¼š 500Âµsï¼ˆå†…å­˜å¸¦å®½ç“¶é¢ˆï¼‰
Flash Attentionï¼š 150Âµsï¼ˆ3x æå‡ï¼‰
```

#### SwiGLU èåˆ

**ä¼ ç»Ÿ SwiGLUï¼ˆ3 ä¸ª kernelï¼‰**ï¼š
```python
gate = W1(x)           # Kernel 1
up = W3(x)             # Kernel 2
output = gate * silu(up)  # Kernel 3
```

**èåˆ SwiGLUï¼ˆ1 ä¸ª kernelï¼‰**ï¼š
```cuda
__global__ void swiglu_kernel(float* output, const float* gate, const float* up) {
    float g = gate[tid];
    float u = up[tid];
    output[tid] = g * (u / (1.0f + expf(-u)));  // èåˆ silu å’Œä¹˜æ³•
}
```

### 3. cuBLASLt è‡ªåŠ¨è°ƒä¼˜

**cuBLASLt** æ˜¯ NVIDIA çš„é«˜çº§çŸ©é˜µä¹˜æ³•åº“ï¼Œæ”¯æŒè‡ªåŠ¨è°ƒä¼˜ï¼š

```rust
fn gemm_cublaslt_bf16(
    a: *const bf16, b: *const bf16, c: *mut bf16,
    M: i32, N: i32, K: i32,
    handle: cublasLtHandle_t,
    workspace: *mut c_void,
) {
    // cuBLASLt ä¼šè‡ªåŠ¨é€‰æ‹©æœ€ä¼˜ç®—æ³•ï¼š
    // - Tensor Core åŠ é€Ÿï¼ˆAmpere+ï¼‰
    // - åˆ†å—ç­–ç•¥
    // - æ˜¯å¦ä½¿ç”¨å…±äº«å†…å­˜
    cublasLtMatmul(handle, ...);
}
```

**æ€§èƒ½**ï¼š
```
æ‰‹å†™ CUDA kernelï¼š ~50% å³°å€¼ TFLOPS
cuBLAS:           ~80% å³°å€¼ TFLOPS
cuBLASLt:         ~90% å³°å€¼ TFLOPSï¼ˆè‡ªåŠ¨è°ƒä¼˜åï¼‰
```

### 4. CUDA Stream å’Œå¼‚æ­¥æ‰§è¡Œ

```rust
pub struct CudaConfig {
    pub stream: cudaStream_t,  // æ¯ä¸ªæ¨¡å‹æœ‰ç‹¬ç«‹æµ
}

impl Llama3 {
    fn forward(&mut self, x: &Tensor) -> Result<Tensor> {
        let stream = self.cuda_config.as_ref().unwrap().stream;

        // æ‰€æœ‰æ“ä½œåœ¨åŒä¸€ä¸ªæµä¸­æ’é˜Ÿï¼ˆè‡ªåŠ¨æµæ°´çº¿ï¼‰
        self.wq_layers[0].forward_async(x, stream)?;
        self.wk_layers[0].forward_async(x, stream)?;  // å¯èƒ½ä¸ä¸Šä¸€è¡Œå¹¶è¡Œ

        // åªåœ¨æœ€ååŒæ­¥ä¸€æ¬¡
        cuda_check!(cudaStreamSynchronize(stream))?;
    }
}
```

---

## å¼€å‘æŒ‡å—

### ç¯å¢ƒè®¾ç½®

```bash
# CPU å¼€å‘
cargo build --release

# CUDA å¼€å‘ï¼ˆéœ€è¦ CUDA 11.8+ï¼‰
export CUDA_PATH=/usr/local/cuda
cargo build --release --features cuda

# è¿è¡Œæµ‹è¯•
cargo test
cargo test --features cuda -- --test-threads=1  # CUDA æµ‹è¯•éœ€è¦ä¸²è¡Œ
```

### æ·»åŠ æ–°ç®—å­

1. **å®šä¹‰ç®—å­ç»“æ„**ï¼š

```rust
// crates/infer-core/src/op/my_op.rs
pub struct MyOp {
    pub weight: Tensor,  // å¦‚æœæœ‰æƒé‡
    pub config: MyOpConfig,
}

impl Op for MyOp {
    fn name(&self) -> &'static str { "MyOp" }

    fn forward(&self, ctx: &mut OpContext) -> Result<()> {
        let input = ctx.inputs[0];
        let output = ctx.outputs[0];

        match input.device() {
            DeviceType::Cpu => kernels::cpu::my_op(input, output),
            DeviceType::Cuda(_) => kernels::cuda::my_op(input, output, ctx.cuda_config),
        }
    }
}
```

2. **å®ç° CPU kernel**ï¼š

```rust
// crates/infer-core/src/op/kernels/cpu/my_op.rs
pub fn my_op(input: &Tensor, output: &mut Tensor) -> Result<()> {
    let input_slice = input.as_f32()?.as_slice()?;
    let output_slice = output.as_f32_mut()?.as_slice_mut()?;

    for (i, o) in input_slice.iter().zip(output_slice.iter_mut()) {
        *o = your_computation(*i);
    }

    Ok(())
}
```

3. **å®ç° CUDA kernel**ï¼š

```cuda
// crates/infer-core/src/op/kernels/cuda/my_op/kernel.cu
extern "C" __global__ void my_op_kernel(
    float* output, const float* input, int n
) {
    int tid = blockIdx.x * blockDim.x + threadIdx.x;
    if (tid < n) {
        output[tid] = your_computation(input[tid]);
    }
}
```

```rust
// crates/infer-core/src/op/kernels/cuda/my_op/mod.rs
extern "C" {
    fn my_op_kernel_cu(
        output: *mut f32, input: *const f32, n: i32, stream: cudaStream_t
    );
}

pub fn my_op(input: &Tensor, output: &mut Tensor, cuda_config: Option<&CudaConfig>) -> Result<()> {
    let n = input.num_elements() as i32;
    let stream = cuda_config.map_or(std::ptr::null_mut(), |c| c.stream);

    unsafe {
        my_op_kernel_cu(output.as_mut_ptr(), input.as_ptr(), n, stream);
    }
    cuda_check!(cudaGetLastError())?;

    Ok(())
}
```

4. **åœ¨ build.rs ä¸­ç¼–è¯‘ CUDA kernel**ï¼š

```rust
// crates/infer-core/build.rs
#[cfg(feature = "cuda")]
fn compile_cuda() {
    cc::Build::new()
        .cuda(true)
        .file("src/op/kernels/cuda/my_op/kernel.cu")
        .compile("my_op");
}
```

### æ·»åŠ æ–°æ¨¡å‹

1. **å®ç° Model trait**ï¼š

```rust
// crates/infer-core/src/model/my_model.rs
pub struct MyModel {
    config: RuntimeModelConfig,
    tokenizer: Box<dyn Tokenizer>,
    layers: Vec<Box<dyn Op>>,
    // ...
}

impl Model for MyModel {
    fn init(&mut self, device: DeviceType) -> Result<()> {
        // åŠ è½½æƒé‡
        let loader = ModelLoader::new(&self.config.model_path, self.config.clone())?;

        // åˆå§‹åŒ–å±‚
        self.layers.push(Box::new(Embedding::new(loader.load_tensor("embed")?)));
        // ...

        // ç§»åŠ¨åˆ°ç›®æ ‡è®¾å¤‡
        if let DeviceType::Cuda(id) = device {
            self.to_cuda(id)?;
        }

        Ok(())
    }

    fn forward(&mut self, input: &Tensor, pos: &Tensor) -> Result<Tensor> {
        let mut x = input.clone();

        for layer in &mut self.layers {
            let mut ctx = OpContext {
                inputs: &[&x],
                outputs: &mut [&mut x],
                cuda_config: self.cuda_config.as_ref(),
            };
            layer.forward(&mut ctx)?;
        }

        Ok(x)
    }

    // ... å…¶ä»–æ–¹æ³•
}
```

2. **æ³¨å†Œæ¨¡å‹**ï¼š

```rust
// crates/infer-core/src/model/mod.rs
pub fn load_model(config: RuntimeModelConfig) -> Result<Box<dyn Model>> {
    match config.model_type.as_str() {
        "llama3" => Ok(Box::new(Llama3::new(config)?)),
        "my_model" => Ok(Box::new(MyModel::new(config)?)),
        _ => Err(Error::UnsupportedModel(config.model_type)),
    }
}
```

### ä»£ç è§„èŒƒ

```bash
# æ ¼å¼åŒ–
cargo fmt

# Lint æ£€æŸ¥
cargo clippy -- -D warnings

# ä¿®å¤å¸¸è§é—®é¢˜
cargo clippy --fix
```

**å‘½åçº¦å®š**ï¼š
- ç±»å‹ï¼š`PascalCase`ï¼ˆå¦‚ `TypedTensor`ï¼‰
- å‡½æ•°ï¼š`snake_case`ï¼ˆå¦‚ `forward_async`ï¼‰
- å¸¸é‡ï¼š`SCREAMING_SNAKE_CASE`ï¼ˆå¦‚ `MAX_SEQ_LEN`ï¼‰
- ç”Ÿå‘½å‘¨æœŸï¼š`'çŸ­åç§°'`ï¼ˆå¦‚ `'a`, `'b`ï¼‰

---

## è°ƒè¯•ä¸åˆ†æ

### 1. CUDA é”™è¯¯è°ƒè¯•

```rust
// å¯ç”¨ CUDA é”™è¯¯æ£€æŸ¥
std::env::set_var("CUDA_LAUNCH_BLOCKING", "1");

// è·å–è¯¦ç»†é”™è¯¯ä¿¡æ¯
cuda_check!(cudaGetLastError())?;
cuda_check!(cudaDeviceSynchronize())?;  // å¼ºåˆ¶åŒæ­¥ï¼Œæš´éœ²å¼‚æ­¥é”™è¯¯
```

### 2. æ€§èƒ½åˆ†æ

**ä½¿ç”¨ NVIDIA Nsight Systems**ï¼š

```bash
# ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
nsys profile --trace=cuda,nvtx cargo run --release --features cuda

# åœ¨ UI ä¸­æŸ¥çœ‹
nsight-sys report.nsys-rep
```

**åœ¨ä»£ç ä¸­æ·»åŠ æ ‡è®°**ï¼š

```rust
#[cfg(feature = "cuda")]
use nvtx::*;

impl Llama3 {
    fn forward(&mut self, x: &Tensor) -> Result<Tensor> {
        let _range = nvtx::range!("Llama3::forward");  // åœ¨ Nsight ä¸­æ˜¾ç¤º

        for layer in 0..self.config.n_layers {
            let _layer_range = nvtx::range!("Layer {}", layer);
            // ...
        }
    }
}
```

### 3. å†…å­˜æ³„æ¼æ£€æµ‹

```bash
# ä½¿ç”¨ valgrindï¼ˆä»… CPUï¼‰
valgrind --leak-check=full ./target/release/infer-server

# ä½¿ç”¨ CUDA-MEMCHECKï¼ˆGPUï¼‰
cuda-memcheck --leak-check full ./target/release/infer-server
```

### 4. å•å…ƒæµ‹è¯•

```rust
#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_rmsnorm_cpu() {
        let input = Tensor::from_slice(&[1.0, 2.0, 3.0], &[1, 3], DeviceType::Cpu).unwrap();
        let weight = Tensor::ones(&[3], DataType::F32, DeviceType::Cpu).unwrap();
        let mut output = Tensor::zeros(&[1, 3], DataType::F32, DeviceType::Cpu).unwrap();

        let op = RMSNorm { weight, dim: 3 };
        let mut ctx = OpContext {
            inputs: &[&input],
            outputs: &mut [&mut output],
            cuda_config: None,
        };

        op.forward(&mut ctx).unwrap();

        // éªŒè¯è¾“å‡º
        let expected = ...;
        assert_approx_eq!(output.as_f32().unwrap().as_slice().unwrap(), &expected);
    }

    #[test]
    #[cfg(feature = "cuda")]
    fn test_rmsnorm_cuda() {
        // ç±»ä¼¼ CPU æµ‹è¯•ï¼Œä½†ä½¿ç”¨ DeviceType::Cuda(0)
    }
}
```

---

## æ€»ç»“

RustInfer çš„è®¾è®¡å±•ç¤ºäº† Rust åœ¨ç³»ç»Ÿç¼–ç¨‹ä¸­çš„ä¼˜åŠ¿ï¼š

### æ ¸å¿ƒè®¾è®¡ç†å¿µ

1. **RAII æ¶ˆé™¤æ‰‹åŠ¨èµ„æºç®¡ç†**
   - ä¸å¯èƒ½å¿˜è®°é‡Šæ”¾å†…å­˜
   - ä¸å¯èƒ½åŒé‡é‡Šæ”¾
   - å¼‚å¸¸å®‰å…¨ï¼ˆpanic-safeï¼‰

2. **ç±»å‹ç³»ç»Ÿé˜²æ­¢è¿è¡Œæ—¶é”™è¯¯**
   - è®¾å¤‡ç±»å‹ä¸åŒ¹é…åœ¨ç¼–è¯‘æ—¶æ•è·
   - æ•°æ®ç±»å‹ä¸åŒ¹é…åœ¨ç¼–è¯‘æ—¶æ•è·
   - ç”Ÿå‘½å‘¨æœŸé”™è¯¯åœ¨ç¼–è¯‘æ—¶æ•è·

3. **é›¶æˆæœ¬æŠ½è±¡**
   - Trait å’Œæ³›å‹åœ¨ç¼–è¯‘åæ¶ˆå¤±
   - ä¸æ‰‹å†™ C ä»£ç æ€§èƒ½ç›¸å½“
   - æ›´é«˜çš„å¯ç»´æŠ¤æ€§å’Œå¯è¯»æ€§

4. **å†…å­˜å®‰å…¨**
   - æ²¡æœ‰æ‚¬å‚æŒ‡é’ˆ
   - æ²¡æœ‰æ•°æ®ç«äº‰
   - æ²¡æœ‰ç¼“å†²åŒºæº¢å‡º

### æ€§èƒ½ä¼˜åŒ–ç­–ç•¥

1. **é¢„åˆ†é… workspace**ï¼šæ¶ˆé™¤æ¨ç†å¾ªç¯ä¸­çš„åˆ†é…
2. **å†…å­˜æ± åŒ–**ï¼šå°† CUDA åˆ†é…å¼€é”€ä» 800Âµs é™ä½åˆ° 1Âµs
3. **é›¶æ‹·è´**ï¼šæƒé‡åŠ è½½ã€KV cache åˆ‡ç‰‡ã€tensor view
4. **BF16 æ¨ç†**ï¼š2x å†…å­˜å¸¦å®½ï¼Œ2x ååé‡
5. **Fused kernels**ï¼šFlash Attentionã€SwiGLU
6. **cuBLASLt è°ƒä¼˜**ï¼šè¾¾åˆ° 90% å³°å€¼ TFLOPS

### ä¸å…¶ä»–æ¡†æ¶å¯¹æ¯”

| ç‰¹æ€§ | RustInfer | PyTorch | TensorRT |
|------|-----------|---------|----------|
| å†…å­˜å®‰å…¨ | âœ… ç¼–è¯‘æ—¶ä¿è¯ | âŒ è¿è¡Œæ—¶é”™è¯¯ | âŒ C++ æ‰‹åŠ¨ç®¡ç† |
| é›¶æ‹·è´åŠ è½½ | âœ… mmap | âŒ æ‹·è´ | âœ… mmap |
| è‡ªåŠ¨èµ„æºæ¸…ç† | âœ… RAII | âš ï¸ GC | âš ï¸ æ‰‹åŠ¨/æ™ºèƒ½æŒ‡é’ˆ |
| è®¾å¤‡æŠ½è±¡ | âœ… Trait | âœ… Tensor.device | âš ï¸ ç¡¬ç¼–ç  |
| æ€§èƒ½ | ğŸš€ 90% å³°å€¼ | ğŸš€ 85% å³°å€¼ | ğŸš€ 95% å³°å€¼ |

---

## è·¯çº¿å›¾

### çŸ­æœŸç›®æ ‡
- æ”¯æŒæ›´å¤šæ¨¡å‹æ¶æ„ï¼ˆGPTã€BERTã€T5ï¼‰
- INT8/INT4 é‡åŒ–æ¨ç†
- åŠ¨æ€ batch æ¨ç†

### ä¸­æœŸç›®æ ‡
- åˆ†å¸ƒå¼æ¨ç†ï¼ˆTensor Parallelã€Pipeline Parallelï¼‰
- æŠ•æœºè§£ç ï¼ˆSpeculative Decodingï¼‰
- CUDA Graph ä¼˜åŒ–

### é•¿æœŸç›®æ ‡
- å¤šåç«¯æ”¯æŒï¼ˆVulkanã€Metalï¼‰
- è‡ªå®šä¹‰ç®—å­ DSL
- ç«¯åˆ°ç«¯ç¼–è¯‘ä¼˜åŒ–

---

## å‚ä¸è´¡çŒ®

æˆ‘ä»¬æ¬¢è¿ä»»ä½•å½¢å¼çš„è´¡çŒ®ï¼

### å¦‚ä½•å¼€å§‹

1. Fork æœ¬ä»“åº“
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ï¼š`git checkout -b feature/my-feature`
3. æäº¤æ›´æ”¹ï¼š`git commit -m 'Add my feature'`
4. æ¨é€åˆ†æ”¯ï¼š`git push origin feature/my-feature`
5. åˆ›å»º Pull Request

### è´¡çŒ®æŒ‡å—

- æ‰€æœ‰ä»£ç å¿…é¡»é€šè¿‡ `cargo fmt` å’Œ `cargo clippy`
- æ–°åŠŸèƒ½éœ€è¦æ·»åŠ æµ‹è¯•
- æ€§èƒ½ç›¸å…³çš„æ›´æ”¹éœ€è¦æä¾› benchmark æ•°æ®
- CUDA kernel éœ€è¦åŒæ—¶æä¾› CPU å®ç°ï¼ˆç”¨äºæµ‹è¯•ï¼‰

### è·å–å¸®åŠ©

- GitHub Issuesï¼šæŠ¥å‘Š bug æˆ–æå‡ºåŠŸèƒ½è¯·æ±‚
- Discussionsï¼šè®¨è®ºæ¶æ„è®¾è®¡å’Œæœ€ä½³å®è·µ
- Emailï¼šç›´æ¥è”ç³»ç»´æŠ¤è€…

---

æ„Ÿè°¢æ‚¨å¯¹ RustInfer çš„å…´è¶£ï¼æˆ‘ä»¬æœŸå¾…çœ‹åˆ°æ‚¨çš„è´¡çŒ®ã€‚