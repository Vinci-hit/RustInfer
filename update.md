## 思考
1、vLLM的Engine采用三层架构，是为了避免全局锁，很明显在Rust中不用这么麻烦，直接用异步IO即可实现高性能多卡管理。
Q5：evict() 现在是如何做的，有优化空间吗？
现在 SGLang 中的处理方式是 DFS 全量收集叶子节点 + 堆排序，这种实现方式在高并发或长上下文场景下（导致树的叶子节点极多）会有明显的性能问题：

时间复杂度：每次触发 Eviction（通常发生在显存不足需要 Swap out 时），都需要 O(N) 的时间来遍历树收集叶子，以及 O(N) 的时间来建堆（heapify）。标准的 LRU Cache 配合 Hash Map + 双向链表应该是 O(1) 的。

## 更新说明

### 20260111
新增了前端对数学公式的支持，基于wasm的极高渲染性能。

下一步计划：
1、支持多模型，增强代码复用架构设计。
4、支持多模态模型，同时新增前端对多模态的支持。
2、实现KVcache，实现长上下文，参考Sglang的思路。新增显存占用率可视化，碎片化可视化，KVCache使用空间可视化。
3、实现Continuous Batching，基于KVcache。
5、与vllm对比性能。
6、支持量化算子和模型。
7、支持单机多卡大模型部署，支持TP，PP等并行模式。

## 📊 性能基准与版本演进

### 性能提升（vs v0.1.0）

> **测试环境**: H200, Llama-3.2-1B-Instruct, Batch Size=1

| 指标 | v0.1.0 | v0.2.0 | 提升 |
|------|--------|--------|------|
| **Prefill 吞吐量** | ~355 tok/s | ~1052 tok/s | **3x ⬆️** |
| **Decode 吞吐量** | ~220 tok/s | ~436 tok/s | **2x ⬆️** |
| **模型加载时间** | ~15 秒 | ~5 秒 | **3x ⬆️** |
| **显存占用** | ~12GB (FP32) | ~6GB (BF16) | **50% ⬇️** |

### 版本历史

#### v0.4.0 (当前) - 架构升级
**发布日期**: 2026-01

**核心改进**:
- ✨ **进程分离架构**: 将推理引擎拆分为独立进程
  - 新增 `infer-engine` crate（独立GPU进程）
  - 新增 `infer-protocol` crate（通信协议模板）
  - 新增 `infer-server` ZMQ客户端
  - 实现 ROUTER-DEALER socket通信
- 🚀 **ZeroMQ IPC通信**: MessagePack序列化（5-10x小于JSON）
- 🔧 **可靠性提升**: 服务端重启无需重载模型
- 📈 **可扩展性**: 单引擎支持多服务端实例

**架构优势**:
- 隔离性：GPU进程与HTTP服务端独立
- 可维护性：组件职责清晰分离
- 灵活性：可独立扩展和部署

#### v0.3.0 - CUDA Graph优化
**发布日期**: 2026-01

**核心改进**:
- ⚡ **CUDA Graph捕获与回放**
  - Decode阶段自动捕获计算图
  - Kernel启动开销降低
  - 降低GPU空闲时间
- 🔧 实现 `CudaConfig` 管理图生命周期
- 📊 Decode延迟进一步降低

**技术细节**:
- 首次迭代捕获完整计算图
- 后续迭代直接回放（单次GPU调用）
- 消除重复的kernel启动开销

#### v0.2.0 - 性能突破
**发布日期**: 2026-01

**核心改进**:
- 🚀 **BF16混合精度推理**
  - Prefill吞吐量: 355 → 1052 tok/s (**3x**)
  - Decode吞吐量: 220 → 436 tok/s (**2x**)
  - 显存占用: 4GB → 2GB (**50%**)

**优化技术**:
- Flash Attention GQA实现
- 算子融合（SwiGLU等）
- cuBLASLt自动调优

#### v0.1.0 - 初始版本
**发布日期**: 2025-10

**基础功能**:
- ✅ Llama3模型完整推理
- ✅ CPU和CUDA双后端
- ✅ KV缓存管理
- ✅ OpenAI兼容API
- ✅ 基础Transformer算子
- ✅ F32推理支持

**性能基线**:
- Prefill: ~355 tok/s
- Decode: ~220 tok/s
- 加载时间: ~15s
- 显存占用: ~12GB (FP32)

---