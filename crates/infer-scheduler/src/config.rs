//! Configuration Management - é…ç½®ç®¡ç†
//!
//! æä¾›ç»Ÿä¸€çš„é…ç½®ç®¡ç†ç³»ç»Ÿï¼Œæ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶ã€‚
//!
//! # æ¶æ„è¯´æ˜
//!
//! **é‡è¦**: Scheduler å±‚ä¸æŒæœ‰ Tokenizer
//! - Tokenizer å±äº Server/Frontend å±‚ï¼Œè´Ÿè´£ String â†” Token IDs è½¬æ¢
//! - Scheduler åªå¤„ç† Token IDsï¼Œä¸å­—ç¬¦ä¸²è§£è€¦
//! - `model_path` ä»…ç”¨äºï¼š
//!   1. è¯»å–æ¨¡å‹å…ƒæ•°æ®ï¼ˆconfig.json ä¸­çš„ eos_token_id, max_position_embeddings ç­‰ï¼‰
//!   2. ä¼ é€’ç»™ Worker ç”¨äºåŠ è½½æ¨¡å‹æƒé‡
//!
//! # é…ç½®å±‚æ¬¡
//!
//! ```text
//! SchedulerConfig (é¡¶å±‚é…ç½®)
//!   â”œâ”€ NetworkConfig (ç½‘ç»œé…ç½®)
//!   â”œâ”€ ModelConfig (æ¨¡å‹é…ç½®)
//!   â”œâ”€ MemoryConfig (æ˜¾å­˜é…ç½®)
//!   â”œâ”€ SchedulingConfig (è°ƒåº¦ç­–ç•¥é…ç½®)
//!   â”œâ”€ ParallelismConfig (å¹¶è¡Œé…ç½®)
//!   â””â”€ LoggingConfig (æ—¥å¿—é…ç½®)
//! ```

use crate::coordinator::CoordinatorConfig;
use crate::policy::SchedulePolicyConfig;
use anyhow::{Context, Result};
use clap::Parser;
use serde::{Deserialize, Serialize};
use std::path::PathBuf;

/// Scheduler ä¸»é…ç½®
///
/// å¯ä»¥ä»å‘½ä»¤è¡Œå‚æ•°æˆ–é…ç½®æ–‡ä»¶åŠ è½½
#[derive(Debug, Clone, Serialize, Deserialize, Parser)]
#[command(name = "rustinfer-scheduler")]
#[command(about = "RustInfer Scheduler - LLM Inference Scheduler", long_about = None)]
pub struct SchedulerConfig {
    /// ç½‘ç»œé…ç½®
    #[command(flatten)]
    pub network: NetworkConfig,

    /// æ¨¡å‹é…ç½®
    #[command(flatten)]
    pub model: ModelConfig,

    /// æ˜¾å­˜é…ç½®
    #[command(flatten)]
    pub memory: MemoryConfig,

    /// è°ƒåº¦ç­–ç•¥é…ç½®
    #[command(flatten)]
    pub scheduling: SchedulingConfig,

    /// å¹¶è¡Œé…ç½®
    #[command(flatten)]
    pub parallelism: ParallelismConfig,

    /// æ—¥å¿—é…ç½®
    #[command(flatten)]
    pub logging: LoggingConfig,

    /// å¯é€‰ï¼šä»é…ç½®æ–‡ä»¶åŠ è½½
    #[arg(long, value_name = "FILE")]
    #[serde(skip)]
    pub config_file: Option<PathBuf>,
}

/// ç½‘ç»œé…ç½®
#[derive(Debug, Clone, Serialize, Deserialize, Parser)]
pub struct NetworkConfig {
    /// ZeroMQ endpoint for Server/Frontend communication
    ///
    /// Server connects via DEALER, Scheduler binds via ROUTER.
    ///
    /// Examples:
    /// - IPC: "ipc:///tmp/rustinfer.ipc"
    /// - TCP: "tcp://*:5556"
    #[arg(long, default_value = "ipc:///tmp/rustinfer.ipc")]
    pub frontend_endpoint: String,

    /// ZeroMQ endpoint for Worker communication
    ///
    /// Examples:
    /// - IPC: "ipc:///tmp/rustinfer-scheduler.ipc"
    /// - TCP: "tcp://*:5555"
    #[arg(long, default_value = "ipc:///tmp/rustinfer-scheduler.ipc")]
    pub worker_endpoint: String,

    /// Worker RPC timeout (milliseconds)
    #[arg(long, default_value_t = 30000)]
    pub worker_timeout_ms: u64,

    /// Number of workers to wait for
    #[arg(long, default_value_t = 1)]
    pub num_workers: usize,
}

/// æ¨¡å‹é…ç½®
///
/// **æ³¨æ„**: ä¸åŒ…å« Tokenizerï¼ˆTokenizer å±äº Server å±‚ï¼‰
#[derive(Debug, Clone, Serialize, Deserialize, Parser)]
pub struct ModelConfig {
    /// Model path (safetensors format)
    ///
    /// ç”¨é€”ï¼š
    /// 1. è¯»å– config.json å…ƒæ•°æ®ï¼ˆeos_token_id, max_position_embeddingsï¼‰
    /// 2. ä¼ é€’ç»™ Worker åŠ è½½æƒé‡
    #[arg(long)]
    pub model_path: String,

    /// Model dtype (bf16, fp16, fp32)
    #[arg(long, default_value = "bf16")]
    pub dtype: String,

    /// Enable Flash Attention
    #[arg(long, default_value_t = true)]
    pub enable_flash_attn: bool,

    /// Optional custom config overrides (JSON string)
    #[arg(long)]
    pub custom_config: Option<String>,
}

/// æ˜¾å­˜é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize, Parser)]
pub struct MemoryConfig {
    /// Block size (number of tokens per block)
    ///
    /// å…¸å‹å€¼ï¼š16ï¼ˆvLLM é»˜è®¤ï¼‰
    #[arg(long, default_value_t = 16)]
    pub block_size: usize,

    /// Total number of GPU blocks
    ///
    /// ç”±Schedulerç¡®å®šKVCacheçš„å‚æ•°ï¼Œä¸ºäº†é˜²æ­¢ä¸åŒæ˜¾å¡ä¹‹é—´çš„æœ¨æ¡¶æ•ˆåº”ã€‚
    #[arg(long, default_value_t = 0)]
    pub total_blocks: usize,

    /// GPU memory utilization ratio (0.0 - 1.0)
    ///
    /// ç”¨äºè‡ªåŠ¨è®¡ç®— total_blocks
    /// å…¸å‹å€¼ï¼š0.9 è¡¨ç¤ºä½¿ç”¨ 90% GPU æ˜¾å­˜ç”¨äº KV Cache
    #[arg(long, default_value_t = 0.9)]
    pub gpu_memory_utilization: f32,

    /// Enable prefix caching (RadixTree)
    #[arg(long, default_value_t = true)]
    pub enable_prefix_cache: bool,

    /// Prefix cache capacity (max tokens)
    #[arg(long, default_value_t = 100000)]
    pub prefix_cache_capacity: usize,

    /// Enable Copy-on-Write (for Beam Search)
    #[arg(long, default_value_t = false)]
    pub enable_cow: bool,
}

/// è°ƒåº¦ç­–ç•¥é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize, Parser)]
pub struct SchedulingConfig {
    /// Scheduling policy name
    ///
    /// Options: "continuous" (default), "priority", "fair"
    #[arg(long, default_value = "continuous")]
    pub policy: String,

    /// Max batch size (number of concurrent requests)
    #[arg(long, default_value_t = 256)]
    pub max_batch_size: usize,

    /// Max tokens per step (é˜²æ­¢å•æ¬¡è®¡ç®—è¿‡å¤§)
    #[arg(long, default_value_t = 4096)]
    pub max_tokens_per_step: usize,

    /// Enable preemption (æŠ¢å ä½ä¼˜å…ˆçº§è¯·æ±‚)
    #[arg(long, default_value_t = true)]
    pub enable_preemption: bool,

    /// Preemption threshold (free blocks)
    ///
    /// å½“ç©ºé—²å—ä½äºæ­¤å€¼æ—¶è§¦å‘æŠ¢å 
    #[arg(long, default_value_t = 0)]
    pub preemption_threshold: usize,

    /// Enable swap to CPU memory
    #[arg(long, default_value_t = false)]
    pub enable_swap: bool,

    /// Default request priority
    #[arg(long, default_value_t = 0)]
    pub default_priority: i32,

    /// Idle sleep duration (milliseconds)
    ///
    /// å½“æ²¡æœ‰ä»»åŠ¡æ—¶ï¼Œä¸»å¾ªç¯ä¼‘çœ æ—¶é—´
    #[arg(long, default_value_t = 1)]
    pub idle_sleep_ms: u64,
}

/// å¹¶è¡Œé…ç½®
///
/// æ”¯æŒ Tensor Parallel å’Œ Pipeline Parallel
#[derive(Debug, Clone, Serialize, Deserialize, Parser)]
pub struct ParallelismConfig {
    /// Tensor Parallel size
    ///
    /// å°†æ¨¡å‹æƒé‡åˆ‡åˆ†åˆ°å¤šä¸ª GPU
    #[arg(long, default_value_t = 1)]
    pub tp_size: usize,

    /// Pipeline Parallel size
    ///
    /// å°†æ¨¡å‹å±‚åˆ‡åˆ†åˆ°å¤šä¸ª GPU
    #[arg(long, default_value_t = 1)]
    pub pp_size: usize,

    /// Tensor Parallel rank (auto-assigned by Worker)
    #[arg(skip)]
    pub tp_rank: Option<usize>,

    /// Pipeline Parallel rank (auto-assigned by Worker)
    #[arg(skip)]
    pub pp_rank: Option<usize>,
}

/// æ—¥å¿—é…ç½®
#[derive(Debug, Clone, Serialize, Deserialize, Parser)]
pub struct LoggingConfig {
    /// Log level (trace, debug, info, warn, error)
    #[arg(long, default_value = "info")]
    pub log_level: String,

    /// Log format (text, json)
    #[arg(long, default_value = "text")]
    pub log_format: String,

    /// Enable file logging
    #[arg(long, default_value_t = false)]
    pub log_to_file: bool,

    /// Log file path
    #[arg(long, default_value = "/tmp/rustinfer-scheduler.log")]
    pub log_file: String,
}

impl SchedulerConfig {
    /// ä»å‘½ä»¤è¡Œå‚æ•°åŠ è½½é…ç½®
    pub fn from_args() -> Self {
        Self::parse()
    }

    /// ä»é…ç½®æ–‡ä»¶åŠ è½½ï¼ˆYAML æˆ– JSONï¼‰
    pub fn from_file(path: &str) -> Result<Self> {
        let content = std::fs::read_to_string(path)
            .context(format!("Failed to read config file: {}", path))?;

        // æ ¹æ®æ–‡ä»¶åç¼€åˆ¤æ–­æ ¼å¼
        if path.ends_with(".yaml") || path.ends_with(".yml") {
            serde_yaml::from_str(&content)
                .context("Failed to parse YAML config")
        } else if path.ends_with(".json") {
            serde_json::from_str(&content)
                .context("Failed to parse JSON config")
        } else {
            anyhow::bail!("Unsupported config file format (use .yaml, .yml, or .json)")
        }
    }

    /// åŠ è½½é…ç½®ï¼ˆä¼˜å…ˆä½¿ç”¨ --config-fileï¼Œå¦åˆ™ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ï¼‰
    pub fn load() -> Result<Self> {
        let mut config = Self::from_args();

        // å¦‚æœæŒ‡å®šäº†é…ç½®æ–‡ä»¶ï¼Œä»æ–‡ä»¶åŠ è½½å¹¶åˆå¹¶
        if let Some(config_file) = &config.config_file {
            let file_config = Self::from_file(config_file.to_str().unwrap())?;
            config = config.merge_with(file_config);
        }

        // éªŒè¯é…ç½®
        config.validate()?;

        Ok(config)
    }

    /// åˆå¹¶ä¸¤ä¸ªé…ç½®ï¼ˆå‘½ä»¤è¡Œå‚æ•°ä¼˜å…ˆï¼‰
    fn merge_with(self, _file_config: Self) -> Self {
        // å¯¹äºæœªæ˜¾å¼æŒ‡å®šçš„å‚æ•°ï¼Œä½¿ç”¨æ–‡ä»¶ä¸­çš„å€¼
        // è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…å¯ä»¥ç”¨ derive_builder æˆ–å…¶ä»–å·¥å…·
        // TODO: å®ç°æ›´ç²¾ç»†çš„åˆå¹¶é€»è¾‘
        // å½“å‰ç‰ˆæœ¬ï¼šå‘½ä»¤è¡Œå‚æ•°å§‹ç»ˆä¼˜å…ˆ
        self
    }

    /// éªŒè¯é…ç½®
    pub fn validate(&self) -> Result<()> {
        // éªŒè¯ block_size
        if self.memory.block_size == 0 {
            anyhow::bail!("block_size must be greater than 0");
        }

        // éªŒè¯ gpu_memory_utilization
        if self.memory.gpu_memory_utilization <= 0.0
            || self.memory.gpu_memory_utilization > 1.0
        {
            anyhow::bail!("gpu_memory_utilization must be in range (0.0, 1.0]");
        }

        // éªŒè¯ max_batch_size
        if self.scheduling.max_batch_size == 0 {
            anyhow::bail!("max_batch_size must be greater than 0");
        }

        // éªŒè¯å¹¶è¡Œé…ç½®
        if self.parallelism.tp_size == 0 {
            anyhow::bail!("tp_size must be greater than 0");
        }
        if self.parallelism.pp_size == 0 {
            anyhow::bail!("pp_size must be greater than 0");
        }

        let total_workers = self.parallelism.tp_size * self.parallelism.pp_size;
        if self.network.num_workers < total_workers {
            anyhow::bail!(
                "num_workers ({}) must be >= tp_size * pp_size ({})",
                self.network.num_workers,
                total_workers
            );
        }

        // éªŒè¯ model_path
        if self.model.model_path.is_empty() {
            anyhow::bail!("model_path is required");
        }

        // éªŒè¯ log_level
        match self.logging.log_level.as_str() {
            "trace" | "debug" | "info" | "warn" | "error" => {}
            _ => anyhow::bail!(
                "Invalid log_level: {} (must be trace/debug/info/warn/error)",
                self.logging.log_level
            ),
        }

        Ok(())
    }

    /// è½¬æ¢ä¸º CoordinatorConfig
    pub fn to_coordinator_config(&self) -> CoordinatorConfig {
        CoordinatorConfig {
            block_size: self.memory.block_size,
            total_blocks: self.memory.total_blocks,
            enable_prefix_cache: self.memory.enable_prefix_cache,
            enable_cow: self.memory.enable_cow,
            default_priority: self.scheduling.default_priority,
            idle_sleep_ms: self.scheduling.idle_sleep_ms,
        }
    }

    /// è½¬æ¢ä¸º SchedulePolicyConfig
    pub fn to_policy_config(&self) -> SchedulePolicyConfig {
        SchedulePolicyConfig {
            max_batch_size: self.scheduling.max_batch_size,
            max_tokens_per_step: self.scheduling.max_tokens_per_step,
            enable_preemption: self.scheduling.enable_preemption,
            preemption_threshold: self.scheduling.preemption_threshold,
            enable_swap: self.scheduling.enable_swap,
            enable_prefix_cache: self.memory.enable_prefix_cache,
        }
    }

    /// è½¬æ¢ä¸º MemoryConfig (for MemoryManager)
    pub fn to_memory_config(&self) -> crate::memory::MemoryConfig {
        crate::memory::MemoryConfig {
            total_blocks: self.memory.total_blocks,
            block_size: self.memory.block_size,
            enable_cow: self.memory.enable_cow,
            enable_prefix_cache: self.memory.enable_prefix_cache,
            prefix_cache_capacity: Some(self.memory.prefix_cache_capacity),
        }
    }

    /// è¯»å–æ¨¡å‹å…ƒæ•°æ®ï¼ˆä» config.jsonï¼‰
    ///
    /// è¿”å› (eos_token_id, max_position_embeddings, num_layers, num_heads, head_dim)
    pub fn read_model_metadata(&self) -> Result<ModelMetadata> {
        let config_path = format!("{}/config.json", self.model.model_path);
        let content = std::fs::read_to_string(&config_path)
            .context(format!("Failed to read model config.json: {}", config_path))?;

        let json: serde_json::Value = serde_json::from_str(&content)
            .context("Failed to parse config.json")?;

        let eos_token_id = json["eos_token_id"]
            .as_i64()
            .context("Missing eos_token_id in config.json")? as i32;

        let max_position_embeddings = json["max_position_embeddings"]
            .as_u64()
            .context("Missing max_position_embeddings in config.json")? as usize;

        let num_layers = json["num_hidden_layers"]
            .as_u64()
            .context("Missing num_hidden_layers in config.json")? as usize;

        let num_attention_heads = json["num_attention_heads"]
            .as_u64()
            .context("Missing num_attention_heads in config.json")? as usize;

        let hidden_size = json["hidden_size"]
            .as_u64()
            .context("Missing hidden_size in config.json")? as usize;

        let head_dim = hidden_size / num_attention_heads;

        // å¯¹äº GQA æ¨¡å‹ï¼Œå°è¯•è¯»å– num_key_value_heads
        let num_kv_heads = json["num_key_value_heads"]
            .as_u64()
            .map(|x| x as usize)
            .unwrap_or(num_attention_heads);

        Ok(ModelMetadata {
            eos_token_id,
            max_position_embeddings,
            num_layers,
            num_attention_heads,
            num_kv_heads,
            head_dim,
        })
    }

    /// æ‰“å°é…ç½®æ‘˜è¦
    pub fn print_summary(&self) {
        println!("ğŸ“‹ Scheduler Configuration:");
        println!("  Network:");
        println!("    Frontend endpoint: {}", self.network.frontend_endpoint);
        println!("    Worker endpoint: {}", self.network.worker_endpoint);
        println!("    Num workers: {}", self.network.num_workers);
        println!("  Model:");
        println!("    Path: {}", self.model.model_path);
        println!("    Dtype: {}", self.model.dtype);
        println!("  Memory:");
        println!("    Block size: {}", self.memory.block_size);
        // å¯¹ total_blocks=0 åšç‰¹æ®Šæ˜¾ç¤ºï¼Œå› ä¸ºå®ƒä¼šåœ¨ profile åè‡ªåŠ¨è®¡ç®—
        if self.memory.total_blocks == 0 {
            println!("    Total blocks: auto (will be determined after profiling)");
        } else {
            println!("    Total blocks: {}", self.memory.total_blocks);
        }
        println!("    Prefix cache: {}", self.memory.enable_prefix_cache);
        println!("  Scheduling:");
        println!("    Policy: {}", self.scheduling.policy);
        println!("    Max batch size: {}", self.scheduling.max_batch_size);
        println!("    Preemption: {}", self.scheduling.enable_preemption);
        println!("  Parallelism:");
        println!("    TP size: {}", self.parallelism.tp_size);
        println!("    PP size: {}", self.parallelism.pp_size);
        println!("  Logging:");
        println!("    Level: {}", self.logging.log_level);
    }
}

/// æ¨¡å‹å…ƒæ•°æ®
#[derive(Debug, Clone)]
pub struct ModelMetadata {
    pub eos_token_id: i32,
    pub max_position_embeddings: usize,
    pub num_layers: usize,
    pub num_attention_heads: usize,
    pub num_kv_heads: usize,
    pub head_dim: usize,
}

impl Default for SchedulerConfig {
    fn default() -> Self {
        Self {
            network: NetworkConfig::default(),
            model: ModelConfig::default(),
            memory: MemoryConfig::default(),
            scheduling: SchedulingConfig::default(),
            parallelism: ParallelismConfig::default(),
            logging: LoggingConfig::default(),
            config_file: None,
        }
    }
}

impl Default for NetworkConfig {
    fn default() -> Self {
        Self {
            frontend_endpoint: "ipc:///tmp/rustinfer.ipc".to_string(),
            worker_endpoint: "ipc:///tmp/rustinfer-scheduler.ipc".to_string(),
            worker_timeout_ms: 30000,
            num_workers: 1,
        }
    }
}

impl Default for ModelConfig {
    fn default() -> Self {
        Self {
            model_path: String::new(),
            dtype: "bf16".to_string(),
            enable_flash_attn: true,
            custom_config: None,
        }
    }
}

impl Default for MemoryConfig {
    fn default() -> Self {
        Self {
            block_size: 16,
            total_blocks: 1000,
            gpu_memory_utilization: 0.9,
            enable_prefix_cache: true,
            prefix_cache_capacity: 100000,
            enable_cow: false,
        }
    }
}

impl Default for SchedulingConfig {
    fn default() -> Self {
        Self {
            policy: "continuous".to_string(),
            max_batch_size: 256,
            max_tokens_per_step: 4096,
            enable_preemption: true,
            preemption_threshold: 0,
            enable_swap: false,
            default_priority: 0,
            idle_sleep_ms: 1,
        }
    }
}

impl Default for ParallelismConfig {
    fn default() -> Self {
        Self {
            tp_size: 1,
            pp_size: 1,
            tp_rank: None,
            pp_rank: None,
        }
    }
}

impl Default for LoggingConfig {
    fn default() -> Self {
        Self {
            log_level: "info".to_string(),
            log_format: "text".to_string(),
            log_to_file: false,
            log_file: "/tmp/rustinfer-scheduler.log".to_string(),
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;

    #[test]
    fn test_default_config() {
        let config = SchedulerConfig::default();
        assert_eq!(config.memory.block_size, 16);
        assert_eq!(config.scheduling.max_batch_size, 256);
        assert_eq!(config.parallelism.tp_size, 1);
    }

    #[test]
    fn test_config_validation() {
        let mut config = SchedulerConfig::default();
        config.model.model_path = "/tmp/test-model".to_string();

        // Valid config should pass
        assert!(config.validate().is_ok());

        // Invalid block_size
        config.memory.block_size = 0;
        assert!(config.validate().is_err());
        config.memory.block_size = 16;

        // Invalid gpu_memory_utilization
        config.memory.gpu_memory_utilization = 1.5;
        assert!(config.validate().is_err());
        config.memory.gpu_memory_utilization = 0.9;

        // Invalid max_batch_size
        config.scheduling.max_batch_size = 0;
        assert!(config.validate().is_err());
        config.scheduling.max_batch_size = 256;

        // Invalid log_level
        config.logging.log_level = "invalid".to_string();
        assert!(config.validate().is_err());
    }

    #[test]
    fn test_to_coordinator_config() {
        let config = SchedulerConfig::default();
        let coord_config = config.to_coordinator_config();

        assert_eq!(coord_config.block_size, config.memory.block_size);
        assert_eq!(coord_config.total_blocks, config.memory.total_blocks);
        assert_eq!(
            coord_config.enable_prefix_cache,
            config.memory.enable_prefix_cache
        );
    }

    #[test]
    fn test_to_policy_config() {
        let config = SchedulerConfig::default();
        let policy_config = config.to_policy_config();

        assert_eq!(
            policy_config.max_batch_size,
            config.scheduling.max_batch_size
        );
        assert_eq!(
            policy_config.enable_preemption,
            config.scheduling.enable_preemption
        );
    }
}
