//! RustInfer Scheduler - ä¸»æœåŠ¡å…¥å£
//!
//! è´Ÿè´£å¯åŠ¨è°ƒåº¦å™¨æœåŠ¡ï¼Œåè°ƒ GPU Workers æ‰§è¡Œæ¨ç†ä»»åŠ¡

use infer_scheduler::config::SchedulerConfig;
use infer_scheduler::coordinator::Coordinator;
use infer_scheduler::policy::ContinuousBatchingPolicy;
use infer_scheduler::transport::{create_frontend_channel, WorkerProxy, ZmqFrontendServer};
use infer_protocol::{ModelLoadParams, ProfileParams, InitKVCacheParams, SchedulerOutput};

use anyhow::{Result, Context};
use tracing::{info, warn, error};
use tracing_subscriber;
use tokio::sync::mpsc;
use std::sync::{Arc, RwLock};
use std::collections::HashMap;

#[tokio::main]
async fn main() -> Result<()> {
    // åŠ è½½é…ç½®ï¼ˆæ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶ï¼‰
    let mut config = SchedulerConfig::load()
        .context("Failed to load configuration")?;

    // åˆå§‹åŒ–æ—¥å¿—
    init_logging(&config.logging)?;

    info!("ğŸš€ RustInfer Scheduler starting...");

    // æ‰“å°é…ç½®æ‘˜è¦
    config.print_summary();

    // è¯»å–æ¨¡å‹å…ƒæ•°æ®ï¼ˆä» config.jsonï¼‰
    info!("ğŸ“– Reading model metadata from config.json...");
    let model_metadata = config.read_model_metadata()
        .context("Failed to read model metadata")?;

    info!(
        "âœ… Model metadata: {} layers, {} heads, eos_token={}",
        model_metadata.num_layers,
        model_metadata.num_attention_heads,
        model_metadata.eos_token_id
    );

    // åˆ›å»ºå‰ç«¯é€šé“ï¼ˆç”¨äºæ¥æ”¶æ¨ç†è¯·æ±‚ï¼‰
    let (frontend_tx, frontend_rx) = create_frontend_channel();

    // åˆ›å»ºå…±äº«çš„è¾“å‡ºè·¯ç”±è¡¨ (request_id -> output_channel)
    // ZmqFrontendServer å’Œ Coordinator å°†å…±äº«è¿™ä¸ª map
    let output_router: Arc<RwLock<HashMap<String, mpsc::UnboundedSender<SchedulerOutput>>>> =
        Arc::new(RwLock::new(HashMap::new()));

    // åˆ›å»ºè¾“å‡ºé€šé“ï¼ˆç”¨äºZMQ Frontendå‘é€å“åº”åˆ°Serverï¼‰
    let (output_tx, _output_rx) = mpsc::unbounded_channel::<SchedulerOutput>();

    // å¯åŠ¨ ZMQ Frontend Server
    info!("ğŸ“¡ Binding ZMQ Frontend to: {}", config.network.frontend_endpoint);
    let zmq_frontend = ZmqFrontendServer::bind(
        config.network.frontend_endpoint.clone(),
        output_tx,
        frontend_tx,
        output_router.clone(),
    )
    .await
    .context("Failed to bind ZMQ Frontend Server")?;

    info!("âœ… ZMQ Frontend Server bound successfully");

    // å¯åŠ¨ ZMQ æ¥æ”¶å¾ªç¯ï¼ˆåœ¨åå°è¿è¡Œï¼‰
    zmq_frontend.start_loop();

    // åˆ›å»º Worker ä»£ç†
    info!("ğŸ“¡ Binding to Worker endpoint: {}", config.network.worker_endpoint);
    let mut worker_proxy = WorkerProxy::new(
        config.network.worker_endpoint.clone(),
        config.network.worker_timeout_ms,
    )
    .await
    .context("Failed to create WorkerProxy")?;

    info!("âœ… Worker endpoint bound successfully: {}", worker_proxy.endpoint());

    // ç­‰å¾… Worker æ³¨å†Œ
    info!("â³ Waiting for {} Worker(s) to register...", config.network.num_workers);
    let mut registered_workers = Vec::new();

    for i in 1..=config.network.num_workers {
        let worker_info = worker_proxy
            .wait_for_registration()
            .await
            .context(format!("Failed to register Worker {}/{}", i, config.network.num_workers))?;

        info!(
            "âœ… Worker {}/{} registered: {} (rank={}, device={}:{})",
            i,
            config.network.num_workers,
            worker_info.worker_id,
            worker_info.rank,
            worker_info.device_type,
            worker_info.device_id
        );

        registered_workers.push(worker_info);
    }

    // å¯¹äºå• Worker æ¨¡å¼ï¼Œç«‹å³åˆå§‹åŒ–
    if config.network.num_workers == 1 {
        let worker_id = registered_workers[0].worker_id.clone();
        info!("ğŸ”§ Initializing single Worker: {}", worker_id);

        // Step 1: åŠ è½½æ¨¡å‹
        info!("ğŸ“¦ Loading model...");
        let model_params = ModelLoadParams {
            device_id: registered_workers[0].device_id,
            model_path: config.model.model_path.clone(),
            dtype: config.model.dtype.clone(),
            tp_rank: 0,
            tp_world_size: config.parallelism.tp_size as u32,
            pp_rank: 0,
            pp_world_size: config.parallelism.pp_size as u32,
            tokenizer_path: None,
            enable_flash_attn: config.model.enable_flash_attn,
            custom_config: config.model.custom_config.clone(),
        };

        let model_info = worker_proxy
            .load_model(&worker_id, model_params)
            .await;

        // Handle load errors robustly - don't crash if Worker fails
        let model_info = match model_info {
            Ok(info) => info,
            Err(e) => {
                error!("Failed to load model on Worker {}: {}", worker_id, e);
                // Worker will be marked as failed through the connection manager
                return Err(e);
            }
        };

        info!(
            "âœ… Model loaded: {:.2} GB, parameters={:.2}B",
            model_info.memory_used as f64 / (1024.0 * 1024.0 * 1024.0),
            model_info.num_parameters as f64 / 1_000_000_000.0
        );

        // Step 2: Profile æ˜¾å­˜
        info!("ğŸ” Profiling GPU memory...");
        let profile_params = ProfileParams {
            batch_size: config.scheduling.max_batch_size,
            seq_len: 2048, // é»˜è®¤åºåˆ—é•¿åº¦
            num_rounds: 3,
            include_prefill: true,
            include_decode: true,
        };

        let profile_result = worker_proxy
            .profile(&worker_id, profile_params)
            .await
            .context("Failed to profile GPU memory")?;

        info!(
            "âœ… Profile completed: {:.2} GB total, {:.2} GB available for KV Cache",
            profile_result.total_memory as f64 / (1024.0 * 1024.0 * 1024.0),
            profile_result.available_kv_cache_memory as f64 / (1024.0 * 1024.0 * 1024.0)
        );

        // Step 2.5: å¦‚æœ total_blocks=0ï¼Œæ ¹æ® profile ç»“æœè‡ªåŠ¨è®¡ç®—
        let total_blocks = if config.memory.total_blocks == 0 {
            // è®¡ç®—æ¯ä¸ª block éœ€è¦çš„æ˜¾å­˜
            // = block_size * num_layers * num_kv_heads * head_dim * 2 (Kå’ŒV) * dtype_bytes
            let dtype_bytes = match config.model.dtype.as_str() {
                "bf16" | "fp16" => 2,
                "fp32" => 4,
                _ => 2, // é»˜è®¤ bf16
            };

            let bytes_per_block = config.memory.block_size
                * model_metadata.num_layers
                * model_metadata.num_kv_heads
                * model_metadata.head_dim
                * 2  // K å’Œ V
                * dtype_bytes;

            let computed_blocks = profile_result.available_kv_cache_memory as usize / bytes_per_block;

            // åº”ç”¨ gpu_memory_utilization ç³»æ•°
            let final_blocks = (computed_blocks as f32 * config.memory.gpu_memory_utilization) as usize;

            info!(
                "ğŸ“Š Auto-computed total_blocks: {} (from {:.2} GB available memory)",
                final_blocks,
                profile_result.available_kv_cache_memory as f64 / (1024.0 * 1024.0 * 1024.0)
            );

            final_blocks
        } else {
            info!("ğŸ“Š Using user-specified total_blocks: {}", config.memory.total_blocks);
            config.memory.total_blocks
        };

        // æ›´æ–° config ä¸­çš„ total_blocks
        config.memory.total_blocks = total_blocks;

        // Step 3: åˆå§‹åŒ– KV Cache
        info!("ğŸ—„ï¸  Initializing KV Cache...");

        // ä½¿ç”¨ä» config.json è¯»å–çš„çœŸå®å‚æ•°
        let kv_cache_params = InitKVCacheParams {
            num_blocks: total_blocks,
            block_size: config.memory.block_size,
            num_layers: model_metadata.num_layers as u32,
            num_heads: model_metadata.num_kv_heads as u32,
            head_dim: model_metadata.head_dim as u32,
            dtype: config.model.dtype.clone(),
            use_unified_memory_pool: true,
        };

        let kv_cache_info = worker_proxy
            .init_kv_cache(&worker_id, kv_cache_params)
            .await
            .context("Failed to initialize KV Cache")?;

        info!(
            "âœ… KV Cache initialized: {} blocks, {} MB",
            kv_cache_info.allocated_blocks,
            kv_cache_info.memory_used / 1024 / 1024
        );

        // å¥åº·æ£€æŸ¥
        if worker_proxy.health_check(&worker_id).await? {
            info!("âœ… Worker health check passed");
        } else {
            warn!("âš ï¸  Worker health check failed");
        }
    } else {
        // å¤š Worker æ¨¡å¼ï¼šTODO æ”¯æŒ Tensor Parallel
        warn!("âš ï¸  Multi-worker mode not yet implemented, using first worker only");
    }

    // åˆ›å»º Coordinator
    info!("ğŸ¯ Creating Coordinator...");

    // ä½¿ç”¨é…ç½®è½¬æ¢æ–¹æ³•
    let policy_config = config.to_policy_config();
    let policy = Box::new(ContinuousBatchingPolicy::new(policy_config));

    let coordinator_config = config.to_coordinator_config();

    let mut coordinator = Coordinator::new(
        policy,
        worker_proxy,
        frontend_rx,
        coordinator_config,
        output_router.clone(),
    );

    // è®¾ç½®é»˜è®¤ Worker ID
    if !registered_workers.is_empty() {
        coordinator.set_default_worker(registered_workers[0].worker_id.clone());
    }

    info!("âœ… Coordinator created successfully");
    info!("ğŸ“Š Statistics:");
    info!("  - Block size: {}", config.memory.block_size);
    info!("  - Total blocks: {}", config.memory.total_blocks);
    info!("  - Memory: {} G", config.memory.total_blocks * config.memory.block_size
                * model_metadata.num_layers
                * model_metadata.num_kv_heads
                * model_metadata.head_dim
                * 2  // K å’Œ V
                * 2 / 1024 / 1024 / 1024); // å‡è®¾ bf16

    // å¯åŠ¨ Coordinator ä¸»å¾ªç¯
    info!("ğŸš€ Starting Coordinator main loop...");
    info!("ğŸ’¡ Scheduler is ready to accept requests");

    // ä½¿ç”¨ tokio::select! æ¥åŒæ—¶ç­‰å¾… coordinator å’Œ shutdown signal
    tokio::select! {
        _ = coordinator.run() => {
            info!("Coordinator exited normally");
        }
        _ = shutdown_signal() => {
            info!("Shutdown signal received, stopping Coordinator...");
        }
    }

    // æ¸…ç†å·¥ä½œ
    info!("Cleaning up resources...");
    drop(zmq_frontend);
    info!("âœ… Scheduler shutdown complete");

    Ok(())
}

/// Graceful Shutdown Signal
async fn shutdown_signal() {
    match tokio::signal::ctrl_c().await {
        Ok(()) => {
            info!("Received Ctrl+C signal");
        }
        Err(err) => {
            error!("Unable to listen for shutdown signal: {}", err);
        }
    }
}

/// åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
fn init_logging(logging_config: &infer_scheduler::config::LoggingConfig) -> Result<()> {
    let filter = match logging_config.log_level.as_str() {
        "trace" => "trace",
        "debug" => "debug",
        "info" => "info",
        "warn" => "warn",
        "error" => "error",
        _ => {
            eprintln!("Invalid log level: {}, using 'info'", logging_config.log_level);
            "info"
        }
    };

    tracing_subscriber::fmt()
        .with_env_filter(filter)
        .with_target(false)
        .with_thread_ids(false)
        .with_file(false)
        .with_line_number(false)
        .init();

    Ok(())
}
