//! RustInfer Scheduler - ä¸»æœåŠ¡å…¥å£
//!
//! è´Ÿè´£å¯åŠ¨è°ƒåº¦å™¨æœåŠ¡ï¼Œåè°ƒ GPU Workers æ‰§è¡ŒæŽ¨ç†ä»»åŠ¡

use infer_scheduler::config::SchedulerConfig;
use infer_scheduler::coordinator::Coordinator;
use infer_scheduler::policy::ContinuousBatchingPolicy;
use infer_scheduler::transport::{create_frontend_channel, WorkerProxy};
use infer_protocol::{ModelLoadParams, ProfileParams, InitKVCacheParams};

use anyhow::{Result, Context};
use tracing::{info, warn};
use tracing_subscriber;

#[tokio::main]
async fn main() -> Result<()> {
    // åŠ è½½é…ç½®ï¼ˆæ”¯æŒå‘½ä»¤è¡Œå‚æ•°å’Œé…ç½®æ–‡ä»¶ï¼‰
    let config = SchedulerConfig::load()
        .context("Failed to load configuration")?;

    // åˆå§‹åŒ–æ—¥å¿—
    init_logging(&config.logging)?;

    info!("ðŸš€ RustInfer Scheduler starting...");

    // æ‰“å°é…ç½®æ‘˜è¦
    config.print_summary();

    // è¯»å–æ¨¡åž‹å…ƒæ•°æ®ï¼ˆä»Ž config.jsonï¼‰
    info!("ðŸ“– Reading model metadata from config.json...");
    let model_metadata = config.read_model_metadata()
        .context("Failed to read model metadata")?;

    info!(
        "âœ… Model metadata: {} layers, {} heads, eos_token={}",
        model_metadata.num_layers,
        model_metadata.num_attention_heads,
        model_metadata.eos_token_id
    );

    // åˆ›å»ºå‰ç«¯é€šé“ï¼ˆç”¨äºŽæŽ¥æ”¶æŽ¨ç†è¯·æ±‚ï¼‰
    let (_frontend_tx, frontend_rx) = create_frontend_channel();

    // åˆ›å»º Worker ä»£ç†
    info!("ðŸ“¡ Binding to Worker endpoint: {}", config.network.worker_endpoint);
    let mut worker_proxy = WorkerProxy::new(
        config.network.worker_endpoint.clone(),
        config.network.worker_timeout_ms,
    )
    .await
    .context("Failed to create WorkerProxy")?;

    info!("âœ… Worker endpoint bound successfully: {}", worker_proxy.endpoint());

    // ç­‰å¾… Worker æ³¨å†Œ
    info!("â³ Waiting for {} Worker(s) to register...", config.network.num_workers);
    let mut registered_workers = Vec::new();

    for i in 1..=config.network.num_workers {
        let worker_info = worker_proxy
            .wait_for_registration()
            .await
            .context(format!("Failed to register Worker {}/{}", i, config.network.num_workers))?;

        info!(
            "âœ… Worker {}/{} registered: {} (rank={}, device={}:{})",
            i,
            config.network.num_workers,
            worker_info.worker_id,
            worker_info.rank,
            worker_info.device_type,
            worker_info.device_id
        );

        registered_workers.push(worker_info);
    }

    // å¯¹äºŽå• Worker æ¨¡å¼ï¼Œç«‹å³åˆå§‹åŒ–
    if config.network.num_workers == 1 {
        let worker_id = registered_workers[0].worker_id.clone();
        info!("ðŸ”§ Initializing single Worker: {}", worker_id);

        // Step 1: åŠ è½½æ¨¡åž‹
        info!("ðŸ“¦ Loading model...");
        let model_params = ModelLoadParams {
            device_id: registered_workers[0].device_id,
            model_path: config.model.model_path.clone(),
            dtype: config.model.dtype.clone(),
            tp_rank: 0,
            tp_world_size: config.parallelism.tp_size as u32,
            pp_rank: 0,
            pp_world_size: config.parallelism.pp_size as u32,
            // TODO: å°† tokenizer_path ä»Ž Scheduler é…ç½®ä¸­ç§»é™¤
            // ç›®å‰ä¿ç•™æ˜¯ä¸ºäº†å…¼å®¹ Worker çš„æŽ¥å£è¦æ±‚
            // æœªæ¥ Worker åº”è¯¥è‡ªå·±ä»Ž model_path åŠ è½½ tokenizer.json
            tokenizer_path: None,
            enable_flash_attn: config.model.enable_flash_attn,
            custom_config: config.model.custom_config.clone(),
        };

        let model_info = worker_proxy
            .load_model(&worker_id, model_params)
            .await
            .context("Failed to load model")?;

        info!(
            "âœ… Model loaded: {} MB, parameters={}",
            model_info.memory_used / 1024 / 1024,
            model_info.num_parameters
        );

        // Step 2: Profile æ˜¾å­˜
        info!("ðŸ” Profiling GPU memory...");
        let profile_params = ProfileParams {
            batch_size: config.scheduling.max_batch_size,
            seq_len: 2048, // é»˜è®¤åºåˆ—é•¿åº¦
            num_rounds: 3,
            include_prefill: true,
            include_decode: true,
        };

        let profile_result = worker_proxy
            .profile(&worker_id, profile_params)
            .await
            .context("Failed to profile GPU memory")?;

        info!(
            "âœ… Profile completed: {} MB total, {} MB available for KV Cache",
            profile_result.total_memory / 1024 / 1024,
            profile_result.available_kv_cache_memory / 1024 / 1024
        );

        // Step 3: åˆå§‹åŒ– KV Cache
        info!("ðŸ—„ï¸  Initializing KV Cache...");

        // ä½¿ç”¨ä»Ž config.json è¯»å–çš„çœŸå®žå‚æ•°
        let kv_cache_params = InitKVCacheParams {
            num_blocks: config.memory.total_blocks,
            block_size: config.memory.block_size,
            num_layers: model_metadata.num_layers as u32,
            num_heads: model_metadata.num_kv_heads as u32,
            head_dim: model_metadata.head_dim as u32,
            dtype: config.model.dtype.clone(),
            use_unified_memory_pool: true,
        };

        let kv_cache_info = worker_proxy
            .init_kv_cache(&worker_id, kv_cache_params)
            .await
            .context("Failed to initialize KV Cache")?;

        info!(
            "âœ… KV Cache initialized: {} blocks, {} MB",
            kv_cache_info.allocated_blocks,
            kv_cache_info.memory_used / 1024 / 1024
        );

        // å¥åº·æ£€æŸ¥
        if worker_proxy.health_check(&worker_id).await? {
            info!("âœ… Worker health check passed");
        } else {
            warn!("âš ï¸  Worker health check failed");
        }
    } else {
        // å¤š Worker æ¨¡å¼ï¼šTODO æ”¯æŒ Tensor Parallel
        warn!("âš ï¸  Multi-worker mode not yet implemented, using first worker only");
    }

    // åˆ›å»º Coordinator
    info!("ðŸŽ¯ Creating Coordinator...");

    // ä½¿ç”¨é…ç½®è½¬æ¢æ–¹æ³•
    let policy_config = config.to_policy_config();
    let policy = Box::new(ContinuousBatchingPolicy::new(policy_config));

    let coordinator_config = config.to_coordinator_config();

    let mut coordinator = Coordinator::new(
        policy,
        worker_proxy,
        frontend_rx,
        coordinator_config,
    );

    // è®¾ç½®é»˜è®¤ Worker ID
    if !registered_workers.is_empty() {
        coordinator.set_default_worker(registered_workers[0].worker_id.clone());
    }

    info!("âœ… Coordinator created successfully");
    info!("ðŸ“Š Statistics:");
    info!("  - Block size: {}", config.memory.block_size);
    info!("  - Total blocks: {}", config.memory.total_blocks);
    info!("  - Memory: {} MB", config.memory.total_blocks * config.memory.block_size * 2 / 1024); // å‡è®¾ bf16

    // å¯åŠ¨ Coordinator ä¸»å¾ªçŽ¯
    info!("ðŸš€ Starting Coordinator main loop...");
    info!("ðŸ’¡ Scheduler is ready to accept requests");

    coordinator.run().await;

    Ok(())
}

/// åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
fn init_logging(logging_config: &infer_scheduler::config::LoggingConfig) -> Result<()> {
    let filter = match logging_config.log_level.as_str() {
        "trace" => "trace",
        "debug" => "debug",
        "info" => "info",
        "warn" => "warn",
        "error" => "error",
        _ => {
            eprintln!("Invalid log level: {}, using 'info'", logging_config.log_level);
            "info"
        }
    };

    tracing_subscriber::fmt()
        .with_env_filter(filter)
        .with_target(false)
        .with_thread_ids(false)
        .with_file(false)
        .with_line_number(false)
        .init();

    Ok(())
}
