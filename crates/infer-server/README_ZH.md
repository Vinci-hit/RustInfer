# infer-server

RustInfer 的 HTTP API 服务端 —— 这是一个高性能的 LLM 推理平台。该组件提供兼容 OpenAI 的 REST API 接口，并具备完善的监控与指标统计功能。

## 概览

`infer-server` 是 RustInfer 平台的 API 网关组件。其核心功能包括：
- 暴露兼容 OpenAI 聊天补全（Chat Completion）格式的 HTTP REST API。
- 通过 ZeroMQ 与推理引擎进行通信，实现低延迟的进程间通信（IPC）。
- 提供实时的系统指标监控端点。
- 支持通过 CUDA 进行 GPU 加速。
- 处理请求路由、模板套用以及响应格式化。

## 目录

- [架构](#架构)
- [设计理念](#设计理念)
- [数据流](#数据流)
- [API 文档](#api-文档)
- [组件结构](#组件结构)
- [配置说明](#配置说明)
- [开发指南](#开发指南)
- [部署方案](#部署方案)
- [故障排查](#故障排查)

---

## 架构

### 高层架构图

```
┌─────────────────────────────────────────────────────────────────┐
│                        客户端层 (Client Layer)                   │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────────┐  │
│  │ 浏览器    │  │ cURL     │  │ Python   │  │ 其他 HTTP     │  │
│  │ (前端)    │  │ CLI      │  │ 脚本     │  │ 客户端        │  │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘  └──────┬───────┘  │
└───────┼────────────┼────────────┼─────────────┼────────────┘
        │            │            │             │
        └────────────────────────┴─────────────┘
                      │ HTTP/JSON
                      ▼
┌─────────────────────────────────────────────────────────────────┐
│                      infer-server                               │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  API 层 (基于 Axum)                                      │  │
│  │  ┌──────────────┐  ┌──────────────┐  ┌──────────────┐  │  │
│  │  │ /chat/       │  │ /metrics     │  │ /health      │  │  │
│  │  │ completions  │  │ (指标统计)    │  │ (健康检查)    │  │  │
│  │  └──────┬───────┘  └──────────────┘  └──────────────┘  │  │
│  │         │                                              │  │
│  │  ┌──────▼────────────────────────────────────────┐     │  │
│  │  │  聊天模板引擎 (Chat Template Engine)            │     │  │
│  │  │  (支持 Llama3 等更多模型...)                    │     │  │
│  │  └──────┬────────────────────────────────────────┘     │  │
│  └─────────┼──────────────────────────────────────────────┘  │
│            │ ZMQ 客户端                                      │
└────────────┼────────────────────────────────────────────────┘
             │ ZeroMQ (MessagePack 序列化)
             │ ipc:///tmp/rustinfer.ipc
             ▼
┌─────────────────────────────────────────────────────────────────┐
│                      infer-scheduler (推理引擎)                     │
│  ┌──────────────────────────────────────────────────────────┐  │
│  │  推理核心                                                │  │
│  │  • 模型加载与执行                                        │  │
│  │  • KV 缓存管理 (基数树/RadixTree)                        │  │
│  │  • 请求调度与批处理 (Batching)                           │  │
│  │  • CUDA GPU 加速                                         │  │
│  └──────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────┘
```

### 组件职责

| 组件 | 职责 |
|-----------|----------------|
| **infer-server** | HTTP API 处理、请求路由、聊天模板转换、指标收集 |
| **infer-scheduler** | 模型推理、GPU 执行、缓存管理、批处理调度 |
| **infer-protocol** | 共享通信协议（消息类型定义、序列化逻辑） |
| **infer-worker** | 核心工具库（张量操作、算子内核、模型抽象） |
| **infer-frontend** | 用于聊天交互和监控仪表板的 Web UI |

---

## 设计理念

### 核心原则

1. **进程分离 (Process Separation)**
   - API 服务端和推理引擎运行在独立进程中。
   - 优势：故障隔离、独立扩缩容、高容错性（引擎重启时 API 仍可在线）。

2. **异步优先架构 (Async-First)**
   - 基于 Tokio 异步运行时构建，支持高并发。
   - 所有外部操作采用非阻塞 I/O，高效处理数千个并发请求。

3. **类型安全与内存安全**
   - 利用 Rust 类型系统提供编译时保证。
   - 零成本抽象确保高性能，无垃圾回收（GC）带来的运行时开销。

4. **低延迟通信**
   - 采用 ZeroMQ 搭配 IPC（进程间通信）传输协议。
   - 使用 MessagePack 进行高效的二进制序列化，服务端与引擎间延迟低于 1 毫秒。

5. **OpenAI 兼容性**
   - 作为 OpenAI 聊天补全 API 的掉入式（Drop-in）替代方案。
   - 现有的 OpenAI 客户端无需修改代码即可无缝接入。

6. **原生可观测性 (Observable by Design)**
   - 从设计之初就集成全面的指标收集。
   - 提供实时监控端点，并在响应中嵌入性能统计数据。

### 使用的设计模式

- **服务层模式 (Service Layer)**：ZMQ 客户端作为共享服务通过 Axum 状态（State）传递。
- **请求-响应关联模式 (Request-Response Correlation)**：每个请求分配唯一 ID（UUID v4）以进行异步跟踪。
- **工厂模式 (Factory Pattern)**：根据模型类型动态选择对应的聊天模板。
- **观察者模式 (Observer Pattern)**：在请求生命周期的各阶段收集指标（排队耗时、预填充耗时、解码耗时）。

---

## 数据流

### 请求生命周期

1. **HTTP 请求**：客户端发送 JSON 请求至 `/v1/chat/completions`。
2. **API 处理器处理**：解析 Body，根据模型套用 `Jinja/Chat` 模板，将消息数组转换为单个 Prompt 字符串。
3. **ZMQ 发送**：封装为内部推理请求，通过 MessagePack 序列化发送至引擎。
4. **引擎执行**：
    - **预填充阶段 (Prefill)**：Token 化输入，加载至 KV 缓存。
    - **解码阶段 (Decode)**：迭代生成 Token，检查停止条件。
5. **响应转换**：接收引擎返回的结果，计算性能指标（Tokens Per Second 等），转换为 OpenAI 兼容格式。
6. **HTTP 返回**：返回 200 OK 及其结果。

### 指标收集流程

服务端定期通过 `sysinfo` 获取 CPU 和内存状态，通过 `nvml`（可选）获取 GPU 利用率与温度，并通过 ZMQ 实时查询引擎内部的缓存命中率、队列长度及处理延迟，最后聚合为统一的 `SystemMetrics` 响应。

---

## API 文档

### 基础 URL
```
http://localhost:8000
```

### 端点详解

#### 1. 聊天补全 (Chat Completion)
创建给定聊天对话的模型响应。

- **路径**: `POST /v1/chat/completions`
- **主要参数**:
    - `model`: 模型 ID（如 "llama3"）。
    - `messages`: 消息对象数组（包含 role 和 content）。
    - `stream`: 是否启用流式响应（目前计划中）。
    - `temperature`: 采样温度 (0-2)。

**性能扩展字段 (RustInfer 特有):**
在 OpenAI 标准响应基础上，我们在 `performance` 对象中额外提供：
- `prefill_time_ms`: 预填充阶段耗时。
- `decode_time_ms`: Token 生成阶段耗时。
- `queue_time_ms`: 请求在队列中的等待耗时。
- `tokens_per_second`: 整体生成吞吐量。

#### 2. 获取系统指标 (Get System Metrics)
检索包括 CPU、内存、GPU、推理引擎及缓存统计在内的全面数据。
- **路径**: `GET /v1/metrics`

#### 3. 健康检查与就绪检查
- `GET /health`: 基础服务状态。
- `GET /ready`: 检查是否已成功连接后端推理引擎。

---

## 组件结构

### 目录布局
- `src/main.rs`: 服务启动入口、路由配置及中间件加载。
- `src/api/`: 包含 OpenAI 兼容接口、健康检查和指标收集的处理器逻辑。
- `src/chat/`: 聊天模板系统，负责不同模型的 Prompt 格式化。
- `src/zmq_client.rs`: 基于 ZeroMQ 的异步通信客户端实现。

---

## 配置说明

### 命令行参数与环境变量

| 参数 | 环境变量 | 默认值 | 描述 |
|----------|---------------------|---------|-------------|
| `--host` | `HOST` | `0.0.0.0` | 绑定地址 |
| `--port` | `PORT` | `8000` | 监听端口 |
| `--engine-endpoint` | `ENGINE_ENDPOINT` | `ipc:///tmp/rustinfer.ipc` | 引擎通信端点 |
| `--log-level` | `RUST_LOG` | `info` | 日志级别 |

### 功能标签 (Feature Flags)
- `cuda`: 默认启用，支持 GPU 监控与加速。若在纯 CPU 环境运行，请使用 `--no-default-features`。

---

## 开发指南

### 前置要求
- **Rust 工具链**: 建议最新稳定版。
- **ZeroMQ 开发库**: `libzmq3-dev` (Ubuntu/Debian)。
- **CUDA Toolkit**: (可选，用于 GPU 支持)。

### 本地运行流程
1. 启动推理引擎：`cargo run -p infer-scheduler`
2. 启动 API 服务端：`cargo run -p infer-server`
3. 使用 cURL 或前端界面进行测试。

---

## 部署方案

### 生产环境构建
```bash
cargo build -p infer-server --release
```

### 部署方式
- **Systemd**: 提供了服务配置文件示例，支持开机自启与异常重启。
- **Docker**: 提供多阶段构建 Dockerfile，大幅缩减镜像体积。
- **反向代理**: 建议在生产环境使用 Nginx 进行 SSL 卸载与负载均衡。

---

## 故障排查

### 常见问题
1. **引擎请求超时 (Engine request timeout)**:
   - 检查 `infer-scheduler` 进程是否存活。
   - 确认 ZMQ IPC 路径是否有读写权限。
2. **连接被拒绝 (Connection refused)**:
   - 检查是否存在过时的 `.ipc` 套接字文件，尝试手动删除。
3. **GPU 指标显示为 null**:
   - 确认驱动已正确安装并可通过 `nvidia-smi` 访问。

---

## 架构决策 FAQ

- **为什么要分离服务端和引擎？** 实现故障隔离，引擎崩溃不会导致 API 网关宕机，且支持分布式部署。
- **为什么选择 ZeroMQ 而非 gRPC？** ZeroMQ 在单机 IPC 场景下开销更低，配置更灵活，性能更接近原生内存速度。
- **为什么使用 MessagePack？** 相比 JSON，二进制格式序列化速度更快且 Payload 更小。

---

## 贡献逻辑

在贡献代码时，请确保：
1. 遵循 Rust 命名规范。
2. 保持 OpenAI API 的向后兼容性。
3. 更新相应的 README 文档。
4. 使用 `anyhow` 处理错误并提供有意义的错误描述。

---

## 许可证

隶属于 RustInfer 项目。详见主项目的 LICENSE 文件。