# RustInfer Server - ç”Ÿäº§çº§HTTPæ¨ç†æœåŠ¡å™¨

> ç°ä»£åŒ–çš„é«˜æ€§èƒ½LLMæ¨ç†HTTPæœåŠ¡å™¨ï¼Œæä¾›OpenAIå…¼å®¹APIï¼Œæ¶æ„è®¾è®¡å€Ÿé‰´vLLMå’ŒSGLangã€‚

## ğŸš€ å¿«é€Ÿå¼€å§‹

```bash
# å¯åŠ¨æœåŠ¡å™¨
cargo run --release --bin rustinfer-server -- \
    --model /path/to/llama3 \
    --port 8000 \
    --device cuda:0 \
    --max-tokens 512

# æˆ–ä½¿ç”¨ç¼–è¯‘å¥½çš„äºŒè¿›åˆ¶æ–‡ä»¶
./target/release/rustinfer-server \
    --model /mnt/d/llama3.2_1B_Instruct/Llama-3.2-1B-Instruct \
    --port 8000 \
    --device cuda:0

# ä½¿ç”¨curlæµ‹è¯•
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3",
    "messages": [{"role": "user", "content": "ä½ å¥½ï¼"}],
    "stream": false
  }'
```

## ğŸ“‹ ç›®å½•

- [åŠŸèƒ½ç‰¹æ€§](#åŠŸèƒ½ç‰¹æ€§)
- [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
- [é«˜çº§è®¾è®¡æ¨¡å¼](#é«˜çº§è®¾è®¡æ¨¡å¼)
- [APIå‚è€ƒ](#apiå‚è€ƒ)
- [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
- [æ€§èƒ½ä¸ä¼˜åŒ–](#æ€§èƒ½ä¸ä¼˜åŒ–)
- [å¼€å‘æŒ‡å—](#å¼€å‘æŒ‡å—)
- [è·¯çº¿å›¾](#è·¯çº¿å›¾)

---

## âœ¨ åŠŸèƒ½ç‰¹æ€§

### æ ¸å¿ƒèƒ½åŠ›
- âœ… **OpenAIå…¼å®¹API** - å¯ç›´æ¥æ›¿æ¢OpenAI API
- âœ… **æ€§èƒ½å¯è§‚æµ‹æ€§** - å®æ—¶æŒ‡æ ‡ï¼ˆé¢„å¡«å……/è§£ç æ—¶é—´ã€tokens/ç§’ï¼‰
- âœ… **ç³»ç»Ÿç›‘æ§** - CPUã€GPUã€å†…å­˜æŒ‡æ ‡ç«¯ç‚¹
- âœ… **æœåŠ¡å™¨æ¨é€äº‹ä»¶ï¼ˆSSEï¼‰** - å®æ—¶æµå¼å“åº”
- âœ… **è‡ªåŠ¨å¯¹è¯æ¨¡æ¿** - Llama3æ ¼å¼åŒ…è£…
- âœ… **å¼‚æ­¥è¿è¡Œæ—¶** - åŸºäºAxum + Tokioæ„å»º
- âœ… **çº¿ç¨‹å®‰å…¨æ¨ç†** - Arc<Mutex>å®ç°å¹¶å‘è¯·æ±‚
- âœ… **CUDA Graphå°±ç»ª** - é¢„åˆ†é…å·¥ä½œç©ºé—´ç¼“å†²åŒº
- âœ… **ä¼˜é›…å…³é—­** - æ­£ç¡®çš„èµ„æºæ¸…ç†
- âœ… **CORSæ”¯æŒ** - å¯ç”¨äºWebåº”ç”¨
- âœ… **ç»“æ„åŒ–æ—¥å¿—** - åŸºäºTracingçš„å¯è§‚æµ‹æ€§

### ç”Ÿäº§å°±ç»ª
- ğŸ”’ ç±»å‹å®‰å…¨çš„Rustå®ç°
- ğŸš€ é›¶æ‹·è´å¼ é‡æ“ä½œ
- ğŸ“Š è¯·æ±‚/å“åº”æ—¥å¿—è®°å½•
- ğŸ¯ å¥åº·æ£€æŸ¥ä¸å°±ç»ªæ¢æµ‹ç«¯ç‚¹
- ğŸ”§ ç¯å¢ƒå˜é‡é…ç½®
- ğŸ“¦ å°ä½“ç§¯äºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆ12MB releaseæ„å»ºï¼‰

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### é«˜å±‚æ¦‚è§ˆ

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å®¢æˆ·ç«¯åº”ç”¨ç¨‹åº                        â”‚
â”‚  (curl, Python OpenAI SDK, Webåº”ç”¨ç­‰)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTP/REST
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚               Axum HTTPæœåŠ¡å™¨ (main.rs)                 â”‚
â”‚  â€¢ è·¯ç”±é…ç½®                                             â”‚
â”‚  â€¢ CORSä¸­é—´ä»¶                                           â”‚
â”‚  â€¢ Tracingä¸­é—´ä»¶                                        â”‚
â”‚  â€¢ ä¼˜é›…å…³é—­å¤„ç†å™¨                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚                 â”‚
            â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  APIå¤„ç†å™¨      â”‚   â”‚  å¥åº·æ£€æŸ¥        â”‚
â”‚  (api/)         â”‚   â”‚  (api/health.rs) â”‚
â”‚                 â”‚   â”‚                  â”‚
â”‚ â€¢ openai.rs     â”‚   â”‚ â€¢ /health        â”‚
â”‚   - /v1/chat/   â”‚   â”‚ â€¢ /ready         â”‚
â”‚     completions â”‚   â”‚                  â”‚
â”‚   - /v1/models  â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ â€¢ metrics.rs    â”‚
â”‚   - /v1/metrics â”‚
â”‚     (ç³»ç»Ÿèµ„æº)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          æ¨ç†å¼•æ“ (inference/engine.rs)                 â”‚
â”‚  â€¢ Arc<Mutex<InferenceEngine>> (çº¿ç¨‹å®‰å…¨)              â”‚
â”‚  â€¢ è¯·æ±‚é˜Ÿåˆ—ä¸åºåˆ—åŒ–                                      â”‚
â”‚  â€¢ å¯¹è¯æ¨¡æ¿åº”ç”¨                                          â”‚
â”‚  â€¢ å“åº”æ ¼å¼åŒ–                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                       â”‚
         â–¼                       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å¯¹è¯æ¨¡æ¿       â”‚     â”‚   infer-core     â”‚
â”‚  (chat/)        â”‚     â”‚   (å¤–éƒ¨crate)    â”‚
â”‚                 â”‚     â”‚                  â”‚
â”‚ â€¢ Llama3æ ¼å¼    â”‚     â”‚ â€¢ Llama3æ¨¡å‹     â”‚
â”‚ â€¢ æ¶ˆæ¯åŒ…è£…      â”‚     â”‚ â€¢ CUDAå†…æ ¸       â”‚
â”‚ â€¢ ç³»ç»Ÿæç¤ºè¯    â”‚     â”‚ â€¢ BF16æ¨ç†       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ç»„ä»¶è¯¦è§£

#### 1. **HTTPå±‚** (`main.rs`)
```rust
// ä½¿ç”¨çŠ¶æ€å…±äº«çš„Axumè·¯ç”±å™¨
let app = Router::new()
    .route("/v1/chat/completions", post(chat_completions))
    .with_state(Arc::new(Mutex::new(engine)))  // å…±äº«å¯å˜çŠ¶æ€
    .layer(CorsLayer::new().allow_origin(Any))  // è·¨åŸŸæ”¯æŒ
    .layer(TraceLayer::new_for_http());         // è¯·æ±‚æ—¥å¿—
```

**è®¾è®¡å†³ç­–ï¼š**
- **Axumæ¡†æ¶**ï¼šç°ä»£åŒ–ã€ç¬¦åˆäººä½“å·¥ç¨‹å­¦ã€åŸºäºTokioæ„å»º
- **Arc<Mutex<>>**ï¼šæ¨¡å‹çš„çº¿ç¨‹å®‰å…¨å…±äº«æ‰€æœ‰æƒ
- **Towerä¸­é—´ä»¶**ï¼šå¯ç»„åˆçš„è¯·æ±‚/å“åº”å¤„ç†

#### 2. **APIå±‚** (`api/`)
```
api/
â”œâ”€â”€ mod.rs          # æ¨¡å—å¯¼å‡º
â”œâ”€â”€ openai.rs       # OpenAIå…¼å®¹ç±»å‹ä¸å¤„ç†å™¨
â”œâ”€â”€ metrics.rs      # ç³»ç»ŸæŒ‡æ ‡ç«¯ç‚¹
â””â”€â”€ health.rs       # å­˜æ´»/å°±ç»ªæ¢æµ‹
```

**å…³é”®ç±»å‹** (`openai.rs`):
```rust
pub struct ChatCompletionRequest {
    pub model: String,
    pub messages: Vec<ChatMessage>,
    pub max_tokens: Option<usize>,
    pub stream: bool,
}

pub struct ChatCompletionResponse {
    pub id: String,              // å”¯ä¸€è¯·æ±‚ID
    pub object: String,          // "chat.completion"
    pub created: i64,            // Unixæ—¶é—´æˆ³
    pub model: String,
    pub choices: Vec<ChatChoice>,
    pub usage: Usage,            // Tokenè®¡æ•°ä¸æ€§èƒ½æŒ‡æ ‡
}

// æ€§èƒ½æŒ‡æ ‡ç»“æ„
pub struct Performance {
    pub prefill_ms: u64,              // é¢„å¡«å……æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub decode_ms: u64,               // è§£ç æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
    pub decode_iterations: usize,     // è§£ç è¿­ä»£æ¬¡æ•°
    pub tokens_per_second: f64,       // ç”Ÿæˆé€Ÿåº¦
    pub time_to_first_token_ms: u64,  // é¦–tokenæ—¶é—´
}
```

---

## ğŸ“š APIå‚è€ƒ

### ç«¯ç‚¹åˆ—è¡¨

#### `POST /v1/chat/completions`

åˆ›å»ºå¸¦æœ‰å¯¹è¯ä¸Šä¸‹æ–‡çš„èŠå¤©è¡¥å…¨ã€‚

**è¯·æ±‚ä½“**:
```json
{
  "model": "llama3",
  "messages": [
    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"},
    {"role": "user", "content": "ä»€ä¹ˆæ˜¯Rustï¼Ÿ"}
  ],
  "max_tokens": 150,
  "stream": false
}
```

**å“åº”** (éæµå¼):
```json
{
  "id": "chatcmpl-abc123",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "llama3",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Rustæ˜¯ä¸€é—¨ç³»ç»Ÿç¼–ç¨‹è¯­è¨€..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 0,
    "completion_tokens": 36,
    "total_tokens": 36,
    "performance": {
      "prefill_ms": 183,
      "decode_ms": 118,
      "decode_iterations": 35,
      "tokens_per_second": 29.9,
      "time_to_first_token_ms": 183
    }
  }
}
```

**å“åº”** (æµå¼ï¼Œ`stream: true`):
```
data: {"id":"chatcmpl-abc123","choices":[{"delta":{"content":"Rust "}}]}

data: {"id":"chatcmpl-abc123","choices":[{"delta":{"content":"æ˜¯ "}}]}

data: [DONE]
```

#### `GET /v1/models`

åˆ—å‡ºå¯ç”¨æ¨¡å‹ã€‚

**å“åº”**:
```json
{
  "object": "list",
  "data": [
    {
      "id": "llama3",
      "object": "model",
      "owned_by": "rustinfer"
    }
  ]
}
```

#### `GET /health`

å¥åº·æ£€æŸ¥ç«¯ç‚¹ã€‚

**å“åº”**:
```json
{
  "status": "healthy",
  "service": "rustinfer-server"
}
```

#### `GET /ready`

å°±ç»ªæ¢æµ‹ï¼ˆæ¨¡å‹å·²åŠ è½½ï¼‰ã€‚

**å“åº”**:
```json
{
  "status": "ready",
  "model_loaded": true
}
```

#### `GET /v1/metrics`

ç³»ç»Ÿèµ„æºç›‘æ§æŒ‡æ ‡ã€‚

**å“åº”**:
```json
{
  "cpu": {
    "utilization_percent": 1.1,
    "core_count": 28
  },
  "memory": {
    "used_mb": 2244,
    "total_mb": 15903,
    "available_mb": 13658
  },
  "gpu": {
    "device_id": 0,
    "utilization_percent": 45.2,
    "memory_used_mb": 2500,
    "memory_total_mb": 24576,
    "temperature_celsius": 65.5
  },
  "timestamp": 1767937158
}
```

**æ³¨æ„**ï¼šå½“CUDAç‰¹æ€§æœªå¯ç”¨æˆ–æ— GPUå¯ç”¨æ—¶ï¼Œ`gpu`å­—æ®µä¸º`null`ã€‚

---

## ğŸ’¡ ä½¿ç”¨ç¤ºä¾‹

### Python (OpenAI SDK)

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy"  # ä¸éªŒè¯
)

# éæµå¼
response = client.chat.completions.create(
    model="llama3",
    messages=[
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªRustä¸“å®¶ã€‚"},
        {"role": "user", "content": "ç”¨2å¥è¯è§£é‡Šæ‰€æœ‰æƒã€‚"}
    ],
    max_tokens=100
)
print(response.choices[0].message.content)

# æµå¼
stream = client.chat.completions.create(
    model="llama3",
    messages=[{"role": "user", "content": "å†™ä¸€é¦–å…³äºRustçš„ä¿³å¥"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="", flush=True)
```

### JavaScript (Fetch API)

```javascript
// éæµå¼
const response = await fetch('http://localhost:8000/v1/chat/completions', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: 'llama3',
    messages: [{ role: 'user', content: 'ä½ å¥½ï¼' }],
    stream: false
  })
});

const data = await response.json();
console.log(data.choices[0].message.content);
```

### curl

```bash
# éæµå¼
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3",
    "messages": [{"role": "user", "content": "ä½ å¥½ï¼"}],
    "stream": false
  }'

# æµå¼ï¼ˆ-Nè¡¨ç¤ºæ— ç¼“å†²ï¼‰
curl -N http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "llama3",
    "messages": [{"role": "user", "content": "è®²ä¸ªç¬‘è¯"}],
    "stream": true
  }'
```

---

## ğŸŒ Webå‰ç«¯

`crates/infer-frontend/`æä¾›äº†ä¸€ä¸ªåŸºäºDioxusçš„ç°ä»£åŒ–Webåº”ç”¨ï¼Œå…·å¤‡ä»¥ä¸‹åŠŸèƒ½ï¼š

- **äº¤äº’å¼èŠå¤©ç•Œé¢** - ä¸æ¨¡å‹è¿›è¡Œå¤šè½®å¯¹è¯
- **å®æ—¶æ€§èƒ½æŒ‡æ ‡** - æ˜¾ç¤ºæ¯ä¸ªå“åº”çš„é¢„å¡«å……/è§£ç æ—¶é—´ã€tokens/ç§’
- **ç³»ç»Ÿç›‘æ§ä»ªè¡¨æ¿** - å®æ—¶CPUã€GPUã€å†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆæ¯2ç§’è½®è¯¢/v1/metricsï¼‰
- **å“åº”å¼UI** - Tailwind CSSæ·±è‰²ä¸»é¢˜
- **åŸºäºWASM** - å®Œå…¨åœ¨æµè§ˆå™¨ä¸­è¿è¡Œ

**å¿«é€Ÿå¯åŠ¨**:
```bash
# ç»ˆç«¯1ï¼šå¯åŠ¨åç«¯
cargo run --release --bin rustinfer-server -- \
    --model /path/to/model \
    --port 8000

# ç»ˆç«¯2ï¼šå¯åŠ¨å‰ç«¯
cd crates/infer-frontend
dx serve --port 3000

# æ‰“å¼€æµè§ˆå™¨ï¼šhttp://localhost:3000
```

è¯¦è§`crates/infer-frontend/README.md`ã€‚

---

## âš¡ æ€§èƒ½ä¸ä¼˜åŒ–

### å½“å‰æ€§èƒ½ï¼ˆLlama-3.2-1B-Instructï¼ŒBF16ï¼ŒRTX 4090ï¼‰

- **æ¨¡å‹åŠ è½½æ—¶é—´**ï¼š9.46ç§’
- **æ¨ç†å»¶è¿Ÿ**ï¼š<100ms TTFTï¼ˆé¦–tokenæ—¶é—´ï¼‰
- **ååé‡**ï¼š~30-40 tokens/ç§’ï¼ˆå•è¯·æ±‚ï¼‰
- **å†…å­˜ä½¿ç”¨**ï¼š~2.5GB VRAMï¼ˆæ¨¡å‹ + KVç¼“å­˜ï¼‰

---

## ğŸ› ï¸ å¼€å‘æŒ‡å—

### é¡¹ç›®ç»“æ„

```
infer-server/
â”œâ”€â”€ Cargo.toml          # ä¾èµ–ä¸æ„å»ºé…ç½®
â”œâ”€â”€ README_CN.md        # æœ¬æ–‡ä»¶
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.rs         # æœåŠ¡å™¨å…¥å£ç‚¹
â”‚   â”œâ”€â”€ lib.rs          # å…¬å…±APIï¼ˆç”¨äºæµ‹è¯•ï¼‰
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â”œâ”€â”€ openai.rs   # OpenAIå…¼å®¹å¤„ç†å™¨
â”‚   â”‚   â”œâ”€â”€ metrics.rs  # ç³»ç»ŸæŒ‡æ ‡ç«¯ç‚¹
â”‚   â”‚   â””â”€â”€ health.rs   # å¥åº·æ£€æŸ¥
â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â””â”€â”€ template.rs # å¯¹è¯æ¨¡æ¿å®ç°
â”‚   â”œâ”€â”€ inference/
â”‚   â”‚   â”œâ”€â”€ mod.rs
â”‚   â”‚   â””â”€â”€ engine.rs   # æ¨ç†å¼•æ“åŒ…è£…å™¨
â”‚   â””â”€â”€ config/
â”‚       â”œâ”€â”€ mod.rs
â”‚       â””â”€â”€ server.rs   # æœåŠ¡å™¨é…ç½®
â””â”€â”€ tests/
    â””â”€â”€ integration.rs  # (å¾…å®Œæˆ) ç«¯åˆ°ç«¯æµ‹è¯•
```

### æ„å»º

```bash
# å¼€å‘æ„å»º
cargo build --bin rustinfer-server

# å‘å¸ƒæ„å»ºï¼ˆä¼˜åŒ–ï¼‰
cargo build --release --bin rustinfer-server

# å¸¦æ—¥å¿—è¿è¡Œ
RUST_LOG=debug cargo run --release --bin rustinfer-server -- \
    --model /path/to/model \
    --port 8000
```

### æµ‹è¯•

```bash
# å•å…ƒæµ‹è¯•
cargo test --lib

# é›†æˆæµ‹è¯•ï¼ˆéœ€å…ˆå¯åŠ¨æœåŠ¡å™¨ï¼‰
cargo test --test integration -- --test-threads=1

# æ‰‹åŠ¨æµ‹è¯•
curl http://localhost:8000/health
```

### ç¯å¢ƒå˜é‡

```bash
# æ‰€æœ‰CLIå‚æ•°éƒ½å¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡è®¾ç½®
export MODEL_PATH=/path/to/model
export HOST=0.0.0.0
export PORT=8000
export DEVICE=cuda:0
export MAX_TOKENS=512
export RUST_LOG=info

./rustinfer-server  # ä½¿ç”¨ç¯å¢ƒå˜é‡
```

---

## ğŸ—ºï¸ è·¯çº¿å›¾

### é˜¶æ®µ1ï¼šMVP âœ…ï¼ˆå·²å®Œæˆï¼‰
- [x] OpenAIå…¼å®¹API
- [x] Llama3å¯¹è¯æ¨¡æ¿
- [x] SSEæµå¼ä¼ è¾“
- [x] å¥åº·æ£€æŸ¥
- [x] CORSæ”¯æŒ
- [x] ä¼˜é›…å…³é—­
- [x] **æ€§èƒ½æŒ‡æ ‡**ï¼ˆé¢„å¡«å……/è§£ç æ—¶é—´ã€tokens/ç§’ï¼‰
- [x] **ç³»ç»Ÿç›‘æ§ç«¯ç‚¹**ï¼ˆ/v1/metricsï¼‰
- [x] **Webå‰ç«¯**ï¼ˆåŸºäºDioxusï¼‰

### é˜¶æ®µ2ï¼šæ€§èƒ½ï¼ˆè¿›è¡Œä¸­ï¼‰
- [x] è¯·æ±‚/å“åº”å¯è§‚æµ‹æ€§ âœ…
- [ ] Tokené€ä¸ªæµå¼ä¼ è¾“ï¼ˆéœ€è¦infer-core APIæ”¯æŒï¼‰
- [ ] è¯·æ±‚æ‰¹å¤„ç†ï¼ˆè¿ç»­æ‰¹å¤„ç†ï¼‰
- [ ] CUDA graphé›†æˆ
- [ ] è¯·æ±‚é˜Ÿåˆ—å¯è§†åŒ–

### é˜¶æ®µ3ï¼šåŠŸèƒ½ç‰¹æ€§
- [ ] å¤šæ¨¡å‹æ”¯æŒï¼ˆåŠ è½½/å¸è½½ï¼‰
- [ ] Temperature/top-p/top-ké‡‡æ ·
- [ ] åœæ­¢åºåˆ—
- [ ] Logprobsè¾“å‡º
- [ ] å‡½æ•°è°ƒç”¨API
- [ ] è§†è§‰æ”¯æŒï¼ˆå¤šæ¨¡æ€ï¼‰

### é˜¶æ®µ4ï¼šç”Ÿäº§ç¯å¢ƒ
- [ ] è¯·æ±‚è®¤è¯ï¼ˆAPIå¯†é’¥ï¼‰
- [ ] é€Ÿç‡é™åˆ¶
- [ ] è¯·æ±‚ç¼“å­˜
- [ ] åˆ†å¸ƒå¼æ¨ç†ï¼ˆå¤šGPUï¼‰
- [ ] Kuberneteséƒ¨ç½²æ¸…å•
- [ ] Dockeré•œåƒ
- [ ] è´Ÿè½½å‡è¡¡å™¨æ°´å¹³æ‰©å±•

---

## ğŸ¤ è´¡çŒ®

### ä»£ç é£æ ¼
- æäº¤å‰è¿è¡Œ`cargo fmt`
- è¿è¡Œ`cargo clippy`å¹¶è§£å†³è­¦å‘Š
- ä¸ºé‡è¦äº‹ä»¶æ·»åŠ tracingæ—¥å¿—
- ä½¿ç”¨`///`æ–‡æ¡£æ³¨é‡Šè®°å½•å…¬å…±API

### Pull Requestæµç¨‹
1. Forkæœ¬ä»“åº“
2. åˆ›å»ºåŠŸèƒ½åˆ†æ”¯ï¼ˆ`git checkout -b feature/amazing`ï¼‰
3. æäº¤æ—¶ä½¿ç”¨æè¿°æ€§æ¶ˆæ¯
4. æ¨é€å¹¶å¼€å¯åŒ…å«è¯¦ç»†æè¿°çš„PR

### æ¶æ„å†³ç­–
å¯¹äºé‡å¤§æ›´æ”¹ï¼Œè¯·å…ˆå¼€issueè®¨è®ºï¼š
- æ€§èƒ½å½±å“
- APIå…¼å®¹æ€§
- å†…å­˜ä½¿ç”¨
- çº¿ç¨‹å®‰å…¨

---

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®æ˜¯RustInferçš„ä¸€éƒ¨åˆ†ï¼Œä½¿ç”¨ç›¸åŒçš„è®¸å¯è¯ã€‚

---

## ğŸ™ è‡´è°¢

**å—ä»¥ä¸‹é¡¹ç›®å¯å‘ï¼š**
- [vLLM](https://github.com/vllm-project/vllm) - PagedAttentionä¸è¿ç»­æ‰¹å¤„ç†
- [SGLang](https://github.com/sgl-project/sglang) - ç»“æ„åŒ–ç”Ÿæˆä¸è¿è¡Œæ—¶
- [Axum](https://github.com/tokio-rs/axum) - ç¬¦åˆäººä½“å·¥ç¨‹å­¦çš„Webæ¡†æ¶
- [OpenAI API](https://platform.openai.com/docs/api-reference) - æ ‡å‡†APIè®¾è®¡

**æ„å»ºå·¥å…·ï¼š**
- ğŸ¦€ Rust - æ€§èƒ½ + å®‰å…¨æ€§
- âš¡ Tokio - å¼‚æ­¥è¿è¡Œæ—¶
- ğŸŒ Axum - HTTPæ¡†æ¶
- ğŸ¯ CUDA - é€šè¿‡infer-coreå®ç°GPUåŠ é€Ÿ

---

## ğŸ“ æ”¯æŒ

å¦‚æœ‰é—®é¢˜æˆ–ç–‘é—®ï¼š
- åœ¨GitHubä¸Šå¼€issue
- æŸ¥çœ‹[infer-coreæ–‡æ¡£](../infer-core/README.md)
- æŸ¥çœ‹æœ¬READMEä¸­çš„APIç¤ºä¾‹

**æœåŠ¡å™¨åœ¨è¿è¡Œå—ï¼Ÿ**ä½¿ç”¨`RUST_LOG=debug`æ£€æŸ¥æ—¥å¿—
