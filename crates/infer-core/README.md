## 📰 更新日志

### v0.2.1 (2026-01-10) - 支持CudaGraph

#### 核心改进
- ✨ **CudaGraph 支持**: 将decoding阶段的运算过程改为固定地址
- 优化计算逻辑，减少不必要的copy

### v0.2.0 (2026-01-09) - 性能大幅提升 🚀

#### 核心改进
- ✨ **BF16 支持**: 新增 BFloat16 混合精度推理，显存占用减半
- ⚡ **算子优化**: 重写关键 CUDA kernel，采用更高效的实现策略
  - Flash Attention GQA 采用Cute对bf16数据进行实现
  - cuBLASLt 矩阵乘法自动调优

#### 性能提升（vs v0.1.0）
| 指标 | v0.1.0 | v0.2.0 | 提升 |
|------|--------|--------|------|
| **Prefill 吞吐量** | ~355 tok/s | ~1052 tok/s | **3x** ⬆️ |
| **Decode 吞吐量** | ~220 tok/s | ~436 tok/s | **2x** ⬆️ |
| **模型加载时间** | ~15 秒 | ~5 秒 | **3.0x** ⬆️ |
| **显存占用** | ~12GB (FP32) | ~6GB (BF16) | **50%** ⬇️ |

# 注意事项
1、flashattention采用异步读取实现，使用了cp.async指令，要求Ampere以上的架构。
2、cublas采用了v2后缀，要求新版本cuda